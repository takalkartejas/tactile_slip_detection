{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is demo code on to check how preprocess the images for vgg network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Manage_data():\n",
    "    def __init__(self):\n",
    "        data_dir='/home/rag-tt/workspace/tactile_images/'\n",
    "        self.data_dir= pathlib.Path(data_dir)\n",
    "\n",
    "    def find_no_of_images(self, obj_id):\n",
    "        image_dir = os.path.join(self.data_dir, str(obj_id))\n",
    "        image_dir= pathlib.Path(image_dir)\n",
    "        no_of_images= len(list(image_dir.glob('*.jpg')))\n",
    "        return no_of_images\n",
    "    \n",
    "    \n",
    "\n",
    "    def parse_function_sequential(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [480, 640])  # Adjust size as needed\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image_resized)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "    \n",
    "    def parse_function_vgg(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [224, 224])  # Adjust size as needed\n",
    "            # Convert image to a float32 tensor and preprocess it for VGG16\n",
    "            image = tf.cast(image_resized, tf.float32)\n",
    "            image = preprocess_input(image)\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "    \n",
    "    def load_sequential_data(self, no_of_samples = 600):\n",
    "        file_paths = []\n",
    "        image_paths = []\n",
    "        sequential_image_paths = []\n",
    "        y = []\n",
    "        window_size = 5\n",
    "        for obj_id in range(no_of_samples):\n",
    "            no_of_images = self.find_no_of_images(obj_id)\n",
    "            if no_of_images < 40:\n",
    "                continue\n",
    "            \n",
    "            csv_path = os.path.join(self.data_dir, str(obj_id),'slip_log.csv')\n",
    "            label = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=1, dtype=None, encoding=None)\n",
    "            y.append(label[:-4])\n",
    "            \n",
    "            for img_id in range(no_of_images):\n",
    "                image_path = os.path.join(self.data_dir, str(obj_id), str(img_id)+ '.jpg')\n",
    "                image_paths.append(image_path)\n",
    "            for i in range(0, len(image_paths) - 4):  # Ensuring sequences of 5 images\n",
    "                row = image_paths[i:i+5]\n",
    "                sequential_image_paths.append(row)\n",
    "            image_paths = []\n",
    "\n",
    "        y = np.concatenate(y)\n",
    "        y = np.array(y)\n",
    "\n",
    "        file_paths = np.array(sequential_image_paths)\n",
    "\n",
    "        return file_paths, y\n",
    "    \n",
    "    def create_sequential_dataset(self,file_paths, labels):\n",
    "                # Create a TensorFlow dataset from the file paths and labels\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        \n",
    "        # # Map the function to each sequence of file paths and labels\n",
    "        # dataset = dataset.map(\n",
    "        #     lambda file_paths, label: self.parse_function_sequential(file_paths, label),\n",
    "        #     num_parallel_calls=tf.data.AUTOTUNE\n",
    "        # )\n",
    "        def wrapped_parse_function(filenames, label):\n",
    "            images, label = tf.py_function(func=self.parse_function_vgg, inp=[filenames, label], Tout=[tf.float32, tf.int64])\n",
    "            images.set_shape((5, 224, 224, 3))  # Explicitly set the shape\n",
    "            label.set_shape([])  # Explicitly set the shape for the label\n",
    "            return images, label\n",
    "        # # Map the parse_function to the dataset using tf.py_function\n",
    "        # dataset = dataset.map(lambda file_paths, label: tf.py_function(func=self.parse_function_sequential, inp=[file_paths, label], Tout=[tf.float32, tf.int64]),\n",
    "        #               num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        #     # Use tf.py_function to apply the parse_function\n",
    "        # def wrapped_parse_function(filenames, label):\n",
    "        #     return tf.py_function(func=self.parse_function_sequential, inp=[filenames, label], Tout=[tf.float32, tf.float32])\n",
    "\n",
    "        # dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        dataset = dataset.batch(32)  # Adjust batch size as needed\n",
    "        self.dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    def split_dataset(self, file_paths):\n",
    "        dataset_size = len(file_paths)\n",
    "        train_size = int(0.8 * dataset_size)\n",
    "        validation_size = int(0.2 * dataset_size)\n",
    "        print('datasize', dataset_size)\n",
    "        print('train size',train_size)\n",
    "        print('validation size', validation_size)\n",
    "        self.train_dataset = self.dataset.take(train_size)\n",
    "        self.val_dataset = self.dataset.skip(train_size)\n",
    "    \n",
    "    # Function to count the elements\n",
    "    def count_elements(self,dataset):\n",
    "        return dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "    \n",
    "    def count_elements2(self,dataset):\n",
    "        count = 0\n",
    "        for _ in dataset:\n",
    "            count += 1\n",
    "        return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manage_data2():\n",
    "    def __init__(self):\n",
    "        data_dir='/home/rag-tt/workspace/tactile_images/'\n",
    "        self.data_dir= pathlib.Path(data_dir)\n",
    "        \n",
    "    def find_no_of_images(self, obj_id):\n",
    "        image_dir = os.path.join(self.data_dir, str(obj_id))\n",
    "        image_dir= pathlib.Path(image_dir)\n",
    "        no_of_images= len(list(image_dir.glob('*.jpg')))\n",
    "        return no_of_images\n",
    "    \n",
    "    def load_data(self, no_of_samples = 600):\n",
    "        file_paths = []\n",
    "        image_paths = []\n",
    "        sequential_image_paths = []\n",
    "        y = []\n",
    "        window_size = tune.sequence_of_image\n",
    "        for obj_id in range(no_of_samples):\n",
    "            no_of_images = self.find_no_of_images(obj_id)\n",
    "            if no_of_images < 40:\n",
    "                continue\n",
    "            \n",
    "            csv_path = os.path.join(self.data_dir, str(obj_id),'slip_log.csv')\n",
    "            label = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=1, dtype=None, encoding=None)\n",
    "            y.append(label[:-(tune.sequence_of_image-1)])\n",
    "            \n",
    "            for img_id in range(no_of_images):\n",
    "                image_path = os.path.join(self.data_dir, str(obj_id), str(img_id)+ '.jpg')\n",
    "                image_paths.append(image_path)\n",
    "            for i in range(0, len(image_paths) - (tune.sequence_of_image-1)):  # Ensuring sequences of 5 images\n",
    "                row = image_paths[i:i+tune.sequence_of_image]\n",
    "                sequential_image_paths.append(row)\n",
    "            image_paths = []\n",
    "\n",
    "        y = np.concatenate(y)\n",
    "        self.labels = np.array(y)\n",
    "\n",
    "        self.file_paths = np.array(sequential_image_paths)\n",
    "        \n",
    "    def shuffle_file_paths(self):\n",
    "        # Shuffle the dataset\n",
    "        indices = np.arange(len(self.file_paths))\n",
    "        np.random.shuffle(indices)\n",
    "        self.file_paths = self.file_paths[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "\n",
    "    def create_split_filepaths(self,train=0.7,val=0.2):\n",
    "        dataset_size = len(self.file_paths)\n",
    "        train_size = int(train * dataset_size)\n",
    "\n",
    "        val_size = int(val * dataset_size)\n",
    "        \n",
    "        test_size = dataset_size - train_size - val_size\n",
    "        \n",
    "        \n",
    "        self.train_filepaths = self.file_paths[ : train_size]\n",
    "        self.val_filepaths = self.file_paths[train_size : train_size+val_size]\n",
    "        self.test_filepaths = self.file_paths[train_size+val_size : ]\n",
    "        \n",
    "        self.train_labels = self.labels[ : train_size]\n",
    "        self.val_labels = self.labels[train_size : train_size+val_size]\n",
    "        self.test_labels = self.labels[train_size+val_size : ]\n",
    "        \n",
    "        # Check the sizes of the splits\n",
    "        assert len(self.train_filepaths) == train_size, \"Training set size mismatch\"\n",
    "        assert len(self.val_filepaths) == val_size, \"Validation set size mismatch\"\n",
    "        assert len(self.test_filepaths) == test_size, \"Test set size mismatch\"\n",
    "        assert len(self.train_labels) == train_size, \"Training set size mismatch\"\n",
    "        assert len(self.val_labels) == val_size, \"Validation set size mismatch\"\n",
    "        assert len(self.test_labels) == test_size, \"Test set size mismatch\"\n",
    "        \n",
    "    def parse_function_vgg(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [224, 224])  # Adjust size as needed\n",
    "            # Convert image to a float32 tensor and preprocess it for VGG16\n",
    "            image = tf.cast(image_resized, tf.float32)\n",
    "            image = preprocess_input(image)\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "        \n",
    "    def create_dataset(self,file_paths, labels):\n",
    "                # Create a TensorFlow dataset from the file paths and labels\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        \n",
    "\n",
    "        def wrapped_parse_function(filenames, label):\n",
    "            images, label = tf.py_function(func=self.parse_function_vgg, inp=[filenames, label], Tout=[tf.float32, tf.int64])\n",
    "            images.set_shape((5, 224, 224, 3))  # Explicitly set the shape\n",
    "            label.set_shape([])  # Explicitly set the shape for the label\n",
    "            return images, label\n",
    " \n",
    "        \n",
    "        dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        dataset = dataset.batch(8)  # Adjust batch size as needed\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "        \n",
    "    def create_split_datasets(self):\n",
    "        self.train_dataset = self.create_dataset(self.train_filepaths, self.train_labels)\n",
    "        self.val_dataset = self.create_dataset(self.val_filepaths, self.val_labels)\n",
    "        self.test_dataset = self.create_dataset(self.test_filepaths, self.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VGG16 model with pre-trained weights\n",
    "# vgg_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "# vgg_model = tf.keras.applications.VGG16(weights='imagenet')\n",
    "# Print the summary of the VGG16 model\n",
    "# vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class create_network():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x =0\n",
    "        \n",
    "    def cnn_lstm1(self):\n",
    "\n",
    "        # Define CNN model\n",
    "        cnn_model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(480, 640, 3),kernel_regularizer=l1(tune.regularizaion_const)),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten()  # Flatten the spatial dimensions\n",
    "        ])\n",
    "\n",
    "        \n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64,input_shape=(tune.sequence_of_image, 144768),kernel_regularizer=l1(tune.regularizaion_const) ),\n",
    "            Dense(8, activation='relu', kernel_regularizer=l1(tune.regularizaion_const)),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "        ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(cnn_model, input_shape=(tune.sequence_of_image, 480, 640, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,144768))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        self.model.summary()\n",
    "\n",
    "    def vgg_lstm(self):\n",
    "        # VGG16 model with pre-trained weights\n",
    "        #include top  false remove the final classification layer\n",
    "        vgg_model = tf.keras.applications.VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
    "        # Freeze the VGG16 layers if you don't want to train them\n",
    "        for layer in vgg_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Define CNN model\n",
    "        vgg_model_flatten = Sequential([\n",
    "            vgg_model,\n",
    "            Flatten(),  # Flatten the spatial dimensions\n",
    "            Dense(tune.dense_neurons1, activation='relu'),\n",
    "            Dropout(tune.dropout1)\n",
    "        ])\n",
    "\n",
    "        #25088 is the output of vff_model_flatten\n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64, input_shape=(tune.sequence_of_image, tune.dense_neurons1)),\n",
    "            Dropout(tune.dropout2),  # Dropout layer to prevent overfitting\n",
    "            Dense(tune.dense_neurons2, activation='relu'),\n",
    "            Dropout(tune.dropout3),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "                ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(vgg_model_flatten, input_shape=(tune.sequence_of_image, 224, 224, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,tune.dense_neurons1))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        vgg_model.summary()\n",
    "        vgg_model_flatten.summary()\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        cp = ModelCheckpoint('model_vgg_test/',monitor='val_accuracy',save_best_only=True)\n",
    "            # EarlyStopping callback to stop training when validation accuracy stops improving\n",
    "        es = EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        #tensor board\n",
    "        tb= tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                # Shuffle training dataset before each epoch\n",
    "        # train_dataset_shuffled = train_dataset.shuffle(buffer_size=train_dataset.cardinality(), reshuffle_each_iteration=tune.reshuffle)\n",
    "        self.model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=tune.learning_rate),metrics=['accuracy'])\n",
    "        self.model.fit(train_dataset,validation_data=val_dataset, epochs=tune.epochs, callbacks=[cp,es,accuracy_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_count = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.sequence_of_image = []\n",
    "        self.learning_rate = []\n",
    "        self.reshuffle =  []\n",
    "        self.dropout1 = []\n",
    "        self.dropout2 = []\n",
    "        self.dropout3 = []\n",
    "        self.dropout4 = []\n",
    "        self.regularization_constant = []\n",
    "        self.batch_size = []\n",
    "        self.dense_neurons1 =[]\n",
    "        self.dense_neurons2 =[]\n",
    "        self.no_of_samples = []\n",
    "        self.epochs  = []\n",
    "        \n",
    "    def reset_dict(self):\n",
    "        self.epoch_count = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.sequence_of_image = []\n",
    "        self.learning_rate = []\n",
    "        self.reshuffle =  []\n",
    "        self.dropout1 = []\n",
    "        self.dropout2 = []\n",
    "        self.dropout3 = []\n",
    "        self.dropout4 = []\n",
    "        self.regularization_constant = []\n",
    "        self.batch_size = []\n",
    "        self.dense_neurons1 =[]\n",
    "        self.dense_neurons2 =[]\n",
    "        self.no_of_samples = []\n",
    "        self.epochs  = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_count.append(epoch + 1)\n",
    "        self.train_accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        self.sequence_of_image.append(tune.sequence_of_image)\n",
    "        self.learning_rate.append(tune.learning_rate)\n",
    "        self.reshuffle.append(tune.reshuffle)\n",
    "        self.dropout1.append(tune.dropout1)\n",
    "        self.dropout2.append(tune.dropout2)\n",
    "        self.dropout3.append(tune.dropout3)\n",
    "        self.dropout4.append(tune.dropout4)\n",
    "        self.regularization_constant.append(tune.regularization_constant)\n",
    "        self.batch_size.append(tune.batch_size)\n",
    "        self.dense_neurons1.append(tune.dense_neurons1)\n",
    "        self.dense_neurons2.append(tune.dense_neurons2)\n",
    "        self.no_of_samples.append(tune.no_of_samples)\n",
    "        self.epochs .append(tune.epochs )\n",
    "    def create_accuracy_dataframe(self):\n",
    "        accuracy_df = pd.DataFrame({\n",
    "            'Epoch': self.epoch_count,\n",
    "            'Train_Accuracy': self.train_accuracy,\n",
    "            'Val_Accuracy': self.val_accuracy,\n",
    "            'Sequence_of_Image': self.sequence_of_image,\n",
    "            'Learning_Rate': self.learning_rate,\n",
    "            'Reshuffle': self.reshuffle,\n",
    "            'Dropout1': self.dropout1,\n",
    "            'Dropout2': self.dropout2,\n",
    "            'Dropout3': self.dropout3,\n",
    "            'Dropout4': self.dropout4,\n",
    "            'Regularization_Constant': self.regularization_constant,\n",
    "            'Batch_Size': self.batch_size,\n",
    "            'dense_neurons1': self.dense_neurons1,\n",
    "            'dense_neurons2': self.dense_neurons2,\n",
    "            'no_of_samples':self.no_of_samples,\n",
    "            'epochs':self.epochs \n",
    "        })\n",
    "        return accuracy_df    \n",
    "    def save_to_csv(self, accuracy_df):\n",
    "            # Start with summary1.csv\n",
    "            file_number = 0\n",
    "            while True:\n",
    "                filename = 'summary' + str(file_number) + '.csv'\n",
    "                # Check if the file already exists\n",
    "                if not os.path.isfile(filename):\n",
    "                    break\n",
    "                file_number += 1\n",
    "            # filename = 'model' + str(file_number) + '.h5'\n",
    "            # network.model.save(filename)\n",
    "            accuracy_df.to_csv(filename, index=False)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tuning():\n",
    "    def __init__(self):\n",
    "        self.sequence_of_image_array = [5,6,8,9,10]\n",
    "        self.learning_rate_array = [0.00005,0.00003, 0.00004, 0.00001,0.0000008, 0.000006 ]\n",
    "        self.reshuffle_array=[False, True]\n",
    "        self.regularization_constant_array = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "        self.dense_neurons2_array = [8, 16, 32]\n",
    "        \n",
    "        self.sequence_of_image =  self.sequence_of_image_array[0]\n",
    "        self.learning_rate = self.learning_rate_array[1]\n",
    "        self.reshuffle =  self.reshuffle_array[0]\n",
    "        self.dropout1 = 0.5\n",
    "        self.dropout2 = 0.5\n",
    "        self.dropout3 = 0.5\n",
    "        self.dropout4 = 0.5\n",
    "        self.regularization_constant = 0.001\n",
    "        self.batch_size = 8\n",
    "        self.dense_neurons1 = 64\n",
    "        self.dense_neurons2 = 8\n",
    "        self.csv_id = 0\n",
    "        self.no_of_samples = 50\n",
    "        self.epochs = 40\n",
    "\n",
    "        \n",
    "    def start_training(self):\n",
    "        try:\n",
    "            manage_data2.load_data(no_of_samples=self.no_of_samples)\n",
    "            manage_data2.shuffle_file_paths()\n",
    "            manage_data2.create_split_filepaths()\n",
    "            manage_data2.create_split_datasets()\n",
    "            network.vgg_lstm()\n",
    "            network.train(manage_data2.train_dataset, manage_data2.val_dataset)\n",
    "        \n",
    "        # Ensure accuracy data is saved even if training is interrupted \n",
    "        finally:        \n",
    "            # Create a DataFrame from the accuracy history lists\n",
    "            accuracy_df = accuracy_history.create_accuracy_dataframe()\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            accuracy_history.save_to_csv(accuracy_df)\n",
    "            accuracy_history.reset_dict()                    \n",
    "    def randomize(self):\n",
    "        \n",
    "        for value in self.regularization_constant_array:\n",
    "            self.regularization_constant = value           \n",
    "            self.start_training()\n",
    "        self.regularization_constant = 0.001\n",
    "            \n",
    "        for value in self.learning_rate_array:\n",
    "            self.learning_rate = value           \n",
    "            self.start_training()\n",
    "        self.learning_rate = 0.00001\n",
    "        for value in self.sequence_of_image_array:\n",
    "            self.sequence_of_image = value           \n",
    "            self.start_training()\n",
    "        self.sequence_of_image = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_data = Manage_data()\n",
    "network = create_network()\n",
    "manage_data2 = Manage_data2()\n",
    "tune = tuning()\n",
    "accuracy_history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage_data2.load_data(no_of_samples=10)\n",
    "# manage_data2.shuffle_file_paths()\n",
    "# manage_data2.create_split_filepaths()\n",
    "# print('train_filepaths', manage_data2.train_filepaths.shape)\n",
    "# print('test_filepaths', manage_data2.test_filepaths.shape)\n",
    "# print('val_filepaths', manage_data2.val_filepaths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage_data2.create_split_datasets()\n",
    "# for batch in manage_data2.train_dataset.take(1):  # Take one batch to print its shape\n",
    "#     images_batch, labels_batch = batch\n",
    "#     print(\"Images batch shape:\", images_batch.shape)\n",
    "#     print(\"Labels batch shape:\", labels_batch.shape)\n",
    "# for batch in manage_data2.test_dataset.take(1):  # Take one batch to print its shape\n",
    "#     images_batch, labels_batch = batch\n",
    "#     print(\"Images batch shape:\", images_batch.shape)\n",
    "#     print(\"Labels batch shape:\", labels_batch.shape)\n",
    "# for batch in manage_data2.val_dataset.take(1):  # Take one batch to print its shape\n",
    "#     images_batch, labels_batch = batch\n",
    "#     print(\"Images batch shape:\", images_batch.shape)\n",
    "#     print(\"Labels batch shape:\", labels_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 64)                1605696   \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16320384 (62.26 MB)\n",
      "Trainable params: 1605696 (6.13 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_12 (TimeD  (None, 5, 64)             16320384  \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 5, 64)             0         \n",
      "                                                                 \n",
      " sequential_39 (Sequential)  (None, 1)                 33553     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16353937 (62.39 MB)\n",
      "Trainable params: 1639249 (6.25 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "41/68 [=================>............] - ETA: 4s - loss: 0.6935 - accuracy: 0.6159Training interrupted. Saving current state...\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                1605696   \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16320384 (62.26 MB)\n",
      "Trainable params: 1605696 (6.13 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_13 (TimeD  (None, 5, 64)             16320384  \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " reshape_13 (Reshape)        (None, 5, 64)             0         \n",
      "                                                                 \n",
      " sequential_42 (Sequential)  (None, 1)                 33553     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16353937 (62.39 MB)\n",
      "Trainable params: 1639249 (6.25 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.7165 - accuracy: 0.5387INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 26s 321ms/step - loss: 0.7165 - accuracy: 0.5387 - val_loss: 0.6719 - val_accuracy: 0.6323\n",
      "Epoch 2/5\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.6989 - accuracy: 0.5616INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 21s 314ms/step - loss: 0.7030 - accuracy: 0.5590 - val_loss: 0.6454 - val_accuracy: 0.6387\n",
      "Epoch 3/5\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.5683INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_vgg_test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 21s 315ms/step - loss: 0.6832 - accuracy: 0.5683 - val_loss: 0.6418 - val_accuracy: 0.6645\n",
      "Epoch 4/5\n",
      "68/68 [==============================] - 16s 227ms/step - loss: 0.6818 - accuracy: 0.5517 - val_loss: 0.6797 - val_accuracy: 0.4387\n",
      "Epoch 5/5\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.6754 - accuracy: 0.5840"
     ]
    }
   ],
   "source": [
    "tune.randomize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the model from the .h5 file\n",
    "# model = load_model('model_vgg.h5')\n",
    "# model.evaluate(manage_data2.test_dataset)\n",
    "\n",
    "# # Print the model summary to confirm it was loaded correctly\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
