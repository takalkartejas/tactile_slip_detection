{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is demo code on to check how preprocess the images for vgg network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manage_data():\n",
    "    def __init__(self):\n",
    "        data_dir='/home/rag-tt/'\n",
    "        self.train_data_dir = os.path.join(data_dir,'train_data')\n",
    "        self.test_data_dir = os.path.join(data_dir,'test_data')\n",
    "        self.data_dir= pathlib.Path(data_dir)\n",
    "    \n",
    "    def count_subdirectories(self,directory):\n",
    "        try:\n",
    "            # List all entries in the directory\n",
    "            entries = os.listdir(directory)\n",
    "            \n",
    "            # Filter out the subdirectories\n",
    "            subdirectories = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "            \n",
    "            # Count the subdirectories\n",
    "            num_subdirectories = len(subdirectories)\n",
    "            \n",
    "            print(f\"The directory '{directory}' contains {num_subdirectories} subdirectories.\")\n",
    "            return num_subdirectories\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"The directory '{directory}' does not exist.\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to access '{directory}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "                \n",
    "    def find_no_of_images(self, data_dir,obj_id):\n",
    "        image_dir = os.path.join(data_dir, str(obj_id))\n",
    "        image_dir= pathlib.Path(image_dir)\n",
    "        no_of_images= len(list(image_dir.glob('*.jpg')))\n",
    "        return no_of_images\n",
    "    \n",
    "        def set_threshold_values(self, label, image_paths, csv_path):\n",
    "        # Ensure label and image_paths are numpy arrays\n",
    "        label = np.array(label)\n",
    "        image_paths = np.array(image_paths)\n",
    "        \n",
    "        # Count the total number of zeroes in label\n",
    "        total_zeroes = np.sum(label == 0)\n",
    "        \n",
    "        # Determine the number of zeroes to remove\n",
    "        zeroes_to_remove = max(0, total_zeroes - tune.no_of_nonslip_data)\n",
    "        \n",
    "        # Indices of zero elements\n",
    "        zero_indices = np.where(label == 0)[0]\n",
    "        \n",
    "        # Indices to keep (last self.no_of_nonslip_data zeroes and all ones)\n",
    "        indices_to_keep = np.concatenate((zero_indices[-tune.no_of_nonslip_data:], np.where(label != 0)[0]))\n",
    "        indices_to_keep = np.unique(indices_to_keep)\n",
    "        indices_to_keep = np.sort(indices_to_keep)\n",
    "        \n",
    "        # Create the resulting label array\n",
    "        label_with_few_zeroes = label[indices_to_keep]\n",
    "        \n",
    "        # Remove the same number of elements from the start of image_paths\n",
    "        paths_with_few_zeroes = image_paths[zeroes_to_remove:]\n",
    "        \n",
    "        trimmed_labels = []\n",
    "        trimmed_paths = []\n",
    "        slip_values = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=2, dtype=None, encoding=None)\n",
    "        i = 0\n",
    "        for slip_value in slip_values:\n",
    "            if slip_value < tune.max_labels:\n",
    "                trimmed_labels.append(label_with_few_zeroes[i])\n",
    "                trimmed_paths.append(paths_with_few_zeroes[i])\n",
    "            i += 1\n",
    "        trimmed_labels = np.array(trimmed_labels)\n",
    "        trimmed_paths = np.array(trimmed_paths)\n",
    "        # print('label_with_few_zeroes =', label_with_few_zeroes.shape)\n",
    "        # print('paths_with_few_zeroes=', paths_with_few_zeroes.shape)\n",
    "        # print('trimmed_labels=', trimmed_labels.shape)\n",
    "        # print('trimed_paths=', trimmed_paths.shape)\n",
    "        return trimmed_labels, trimmed_paths\n",
    "    \n",
    "        def check_pattern(self,label):\n",
    "        # Ensure arr is a numpy array\n",
    "        label = np.array(label)\n",
    "        \n",
    "        # Find the first occurrence of 1\n",
    "        first_one_index = np.argmax(label == 1)\n",
    "        \n",
    "        if np.all(label == 0):  # If there's no 1 in the array, ensure all are 0\n",
    "            return\n",
    "        \n",
    "        # Check if there's no 1 in the array\n",
    "        if np.max(label) == 0:\n",
    "            assert np.all(label == 0), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "            return\n",
    "        \n",
    "        # Assert all elements before first_one_index are 0\n",
    "        assert np.all(label[:first_one_index] == 0), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "        \n",
    "        # Assert all elements from first_one_index to the end are 1\n",
    "        assert np.all(label[first_one_index:] == 1), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "    \n",
    "    def create_slip_instant_labels(self, csv_path):\n",
    "        label = []\n",
    "        slip_values = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=2, dtype=None, encoding=None)\n",
    "        for slip_value in slip_values:\n",
    "            if slip_value < tune.slip_instant_labels:\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(1)\n",
    "        return label\n",
    "    \n",
    "    def duplicate_n_balance_data(self, labels, image_paths):\n",
    "        # Convert labels to numpy array for easier manipulation\n",
    "        labels = np.array(labels)\n",
    "        image_paths = np.array(image_paths)\n",
    "\n",
    "        # Get indices of each class\n",
    "        class_0_indices = np.where(labels == 0)[0]\n",
    "        class_1_indices = np.where(labels == 1)[0]\n",
    "            # Check if either class is empty\n",
    "        if len(class_0_indices) == 0 or len(class_1_indices) == 0:\n",
    "            # print(f\"Skipping balancing for {labels} as one of the classes is missing\")\n",
    "            return labels, image_paths\n",
    "    \n",
    "        # Calculate the difference in the number of samples\n",
    "        diff = len(class_0_indices) - len(class_1_indices)\n",
    "\n",
    "        if diff > 0:  # More 0s than 1s\n",
    "            # Randomly duplicate class 1 samples to balance the dataset\n",
    "            additional_indices = np.random.choice(class_1_indices, size=diff, replace=True)\n",
    "            labels = np.concatenate([labels, labels[additional_indices]])\n",
    "            image_paths = np.concatenate([image_paths, image_paths[additional_indices]])\n",
    "        elif diff < 0:  # More 1s than 0s\n",
    "            # Randomly duplicate class 0 samples to balance the dataset\n",
    "            additional_indices = np.random.choice(class_0_indices, size=-diff, replace=True)\n",
    "            labels = np.concatenate([labels, labels[additional_indices]])\n",
    "            image_paths = np.concatenate([image_paths, image_paths[additional_indices]])\n",
    "        # # Shuffle the dataset to mix the duplicated samples\n",
    "        # shuffle_indices = np.arange(len(labels))\n",
    "        # np.random.shuffle(shuffle_indices)\n",
    "        # labels = labels[shuffle_indices]\n",
    "        # image_paths = image_paths[shuffle_indices]\n",
    "\n",
    "        return labels, image_paths    \n",
    "    \n",
    "    def load_data(self, no_of_samples = 600):\n",
    "        file_paths = []\n",
    "        image_paths = []\n",
    "        sequential_image_paths = []\n",
    "        y = []\n",
    "        window_size = tune.sequence_of_image\n",
    "        for obj_id in range(no_of_samples):\n",
    "            no_of_images = self.find_no_of_images(obj_id)\n",
    "            if no_of_images < 40:\n",
    "                continue\n",
    "            \n",
    "            csv_path = os.path.join(self.data_dir, str(obj_id),'slip_log.csv')\n",
    "            label = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=1, dtype=None, encoding=None)\n",
    "            y.append(label[:-(tune.sequence_of_image-1)])\n",
    "            \n",
    "            for img_id in range(no_of_images):\n",
    "                image_path = os.path.join(self.data_dir, str(obj_id), str(img_id)+ '.jpg')\n",
    "                image_paths.append(image_path)\n",
    "            for i in range(0, len(image_paths) - (tune.sequence_of_image-1)):  # Ensuring sequences of 5 images\n",
    "                row = image_paths[i:i+tune.sequence_of_image]\n",
    "                sequential_image_paths.append(row)\n",
    "            image_paths = []\n",
    "\n",
    "        y = np.concatenate(y)\n",
    "        self.labels = np.array(y)\n",
    "\n",
    "        self.file_paths = np.array(sequential_image_paths)\n",
    "        \n",
    "    def shuffle_file_paths(self):\n",
    "        # Shuffle the dataset\n",
    "        indices = np.arange(len(self.file_paths))\n",
    "        np.random.shuffle(indices)\n",
    "        self.file_paths = self.file_paths[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "\n",
    "    def create_split_filepaths(self,train=0.7,val=0.2):\n",
    "        dataset_size = len(self.file_paths)\n",
    "        train_size = int(train * dataset_size)\n",
    "\n",
    "        val_size = int(val * dataset_size)\n",
    "        \n",
    "        test_size = dataset_size - train_size - val_size\n",
    "        \n",
    "        \n",
    "        self.train_filepaths = self.file_paths[ : train_size]\n",
    "        self.val_filepaths = self.file_paths[train_size : train_size+val_size]\n",
    "        self.test_filepaths = self.file_paths[train_size+val_size : ]\n",
    "        \n",
    "        self.train_labels = self.labels[ : train_size]\n",
    "        self.val_labels = self.labels[train_size : train_size+val_size]\n",
    "        self.test_labels = self.labels[train_size+val_size : ]\n",
    "        \n",
    "        # Check the sizes of the splits\n",
    "        assert len(self.train_filepaths) == train_size, \"Training set size mismatch\"\n",
    "        assert len(self.val_filepaths) == val_size, \"Validation set size mismatch\"\n",
    "        assert len(self.test_filepaths) == test_size, \"Test set size mismatch\"\n",
    "        assert len(self.train_labels) == train_size, \"Training set size mismatch\"\n",
    "        assert len(self.val_labels) == val_size, \"Validation set size mismatch\"\n",
    "        assert len(self.test_labels) == test_size, \"Test set size mismatch\"\n",
    "        \n",
    "    def parse_function_vgg(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [224, 224])  # Adjust size as needed\n",
    "            # Convert image to a float32 tensor and preprocess it for VGG16\n",
    "            image = tf.cast(image_resized, tf.float32)\n",
    "            image = preprocess_input(image)\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "        \n",
    "    def create_dataset(self,file_paths, labels):\n",
    "                # Create a TensorFlow dataset from the file paths and labels\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        \n",
    "\n",
    "        def wrapped_parse_function(filenames, label):\n",
    "            images, label = tf.py_function(func=self.parse_function_vgg, inp=[filenames, label], Tout=[tf.float32, tf.int64])\n",
    "            images.set_shape((5, 224, 224, 3))  # Explicitly set the shape\n",
    "            label.set_shape([])  # Explicitly set the shape for the label\n",
    "            return images, label\n",
    " \n",
    "        \n",
    "        dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        dataset = dataset.batch(8)  # Adjust batch size as needed\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "        \n",
    "    def create_split_datasets(self):\n",
    "        self.train_dataset = self.create_dataset(self.train_filepaths, self.train_labels)\n",
    "        self.val_dataset = self.create_dataset(self.val_filepaths, self.val_labels)\n",
    "        self.test_dataset = self.create_dataset(self.test_filepaths, self.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VGG16 model with pre-trained weights\n",
    "# vgg_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "# vgg_model = tf.keras.applications.VGG16(weights='imagenet')\n",
    "# Print the summary of the VGG16 model\n",
    "# vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class create_network():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x =0\n",
    "        \n",
    "    def cnn_lstm1(self):\n",
    "\n",
    "        # Define CNN model\n",
    "        cnn_model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(480, 640, 3),kernel_regularizer=l1(tune.regularizaion_const)),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten()  # Flatten the spatial dimensions\n",
    "        ])\n",
    "\n",
    "        \n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64,input_shape=(tune.sequence_of_image, 144768),kernel_regularizer=l1(tune.regularizaion_const) ),\n",
    "            Dense(8, activation='relu', kernel_regularizer=l1(tune.regularizaion_const)),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "        ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(cnn_model, input_shape=(tune.sequence_of_image, 480, 640, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,144768))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        self.model.summary()\n",
    "\n",
    "    def vgg_lstm(self):\n",
    "        # VGG16 model with pre-trained weights\n",
    "        #include top  false remove the final classification layer\n",
    "        vgg_model = tf.keras.applications.VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
    "        # Freeze the VGG16 layers if you don't want to train them\n",
    "        for layer in vgg_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Define CNN model\n",
    "        vgg_model_flatten = Sequential([\n",
    "            vgg_model,\n",
    "            Flatten(),  # Flatten the spatial dimensions\n",
    "            Dense(tune.dense_neurons1, activation='relu'),\n",
    "            Dropout(tune.dropout1)\n",
    "        ])\n",
    "\n",
    "        #25088 is the output of vff_model_flatten\n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64, input_shape=(tune.sequence_of_image, tune.dense_neurons1)),\n",
    "            Dropout(tune.dropout2),  # Dropout layer to prevent overfitting\n",
    "            Dense(tune.dense_neurons2, activation='relu'),\n",
    "            Dropout(tune.dropout3),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "                ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(vgg_model_flatten, input_shape=(tune.sequence_of_image, 224, 224, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,tune.dense_neurons1))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        vgg_model.summary()\n",
    "        vgg_model_flatten.summary()\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        cp = ModelCheckpoint('model_vgg_test/',monitor='val_accuracy',save_best_only=True)\n",
    "            # EarlyStopping callback to stop training when validation accuracy stops improving\n",
    "        es = EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        #tensor board\n",
    "        tb= tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                # Shuffle training dataset before each epoch\n",
    "        # train_dataset_shuffled = train_dataset.shuffle(buffer_size=train_dataset.cardinality(), reshuffle_each_iteration=tune.reshuffle)\n",
    "        self.model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=tune.learning_rate),metrics=['accuracy'])\n",
    "        self.model.fit(train_dataset,validation_data=val_dataset, epochs=tune.epochs, callbacks=[cp,es,accuracy_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class ml_algorithm():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier(n_estimators=100)\n",
    "        self.pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "         \n",
    "    def extract_hog_features(self,images):\n",
    "        features = []\n",
    "        # images (5,255,255,3)\n",
    "        for img in images:\n",
    "            gray_img = rgb2gray(img)\n",
    "            hog_features = hog(gray_img, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "            reduced_features = self.pca.fit_transform(hog_features)\n",
    "            features.append(reduced_features)\n",
    "            \n",
    "        features = np.array(features)\n",
    "        # features  (5, 26244)\n",
    "        \n",
    "        print('reduced features', features)\n",
    "        return np.array(features)\n",
    "\n",
    "    # Example function to process a batch of sequences\n",
    "    def process_batch(self,batch):\n",
    "        return np.array([self.extract_hog_features(sequence) for sequence in batch])\n",
    "    \n",
    "    def train(self, train_sequences, train_labels, val_sequences, val_labels):\n",
    "        # Flatten the sequence of HOG features\n",
    "        train_features = [sequence.flatten() for sequence in train_sequences]\n",
    "        val_features = [sequence.flatten() for sequence in val_sequences]\n",
    "        \n",
    "        # Train the Random Forest model\n",
    "        self.model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        val_predictions = self.model.predict(val_features)\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_count = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.sequence_of_image = []\n",
    "        self.learning_rate = []\n",
    "        self.reshuffle =  []\n",
    "        self.dropout1 = []\n",
    "        self.dropout2 = []\n",
    "        self.dropout3 = []\n",
    "        self.dropout4 = []\n",
    "        self.regularization_constant = []\n",
    "        self.batch_size = []\n",
    "        self.dense_neurons1 =[]\n",
    "        self.dense_neurons2 =[]\n",
    "        self.no_of_samples = []\n",
    "        self.epochs  = []\n",
    "        \n",
    "    def reset_dict(self):\n",
    "        self.epoch_count = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.sequence_of_image = []\n",
    "        self.learning_rate = []\n",
    "        self.reshuffle =  []\n",
    "        self.dropout1 = []\n",
    "        self.dropout2 = []\n",
    "        self.dropout3 = []\n",
    "        self.dropout4 = []\n",
    "        self.regularization_constant = []\n",
    "        self.batch_size = []\n",
    "        self.dense_neurons1 =[]\n",
    "        self.dense_neurons2 =[]\n",
    "        self.no_of_samples = []\n",
    "        self.epochs  = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_count.append(epoch + 1)\n",
    "        self.train_accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        self.sequence_of_image.append(tune.sequence_of_image)\n",
    "        self.learning_rate.append(tune.learning_rate)\n",
    "        self.reshuffle.append(tune.reshuffle)\n",
    "        self.dropout1.append(tune.dropout1)\n",
    "        self.dropout2.append(tune.dropout2)\n",
    "        self.dropout3.append(tune.dropout3)\n",
    "        self.dropout4.append(tune.dropout4)\n",
    "        self.regularization_constant.append(tune.regularization_constant)\n",
    "        self.batch_size.append(tune.batch_size)\n",
    "        self.dense_neurons1.append(tune.dense_neurons1)\n",
    "        self.dense_neurons2.append(tune.dense_neurons2)\n",
    "        self.no_of_samples.append(tune.no_of_samples)\n",
    "        self.epochs .append(tune.epochs )\n",
    "    def create_accuracy_dataframe(self):\n",
    "        accuracy_df = pd.DataFrame({\n",
    "            'Epoch': self.epoch_count,\n",
    "            'Train_Accuracy': self.train_accuracy,\n",
    "            'Val_Accuracy': self.val_accuracy,\n",
    "            'Sequence_of_Image': self.sequence_of_image,\n",
    "            'Learning_Rate': self.learning_rate,\n",
    "            'Reshuffle': self.reshuffle,\n",
    "            'Dropout1': self.dropout1,\n",
    "            'Dropout2': self.dropout2,\n",
    "            'Dropout3': self.dropout3,\n",
    "            'Dropout4': self.dropout4,\n",
    "            'Regularization_Constant': self.regularization_constant,\n",
    "            'Batch_Size': self.batch_size,\n",
    "            'dense_neurons1': self.dense_neurons1,\n",
    "            'dense_neurons2': self.dense_neurons2,\n",
    "            'no_of_samples':self.no_of_samples,\n",
    "            'epochs':self.epochs \n",
    "        })\n",
    "        return accuracy_df    \n",
    "    def save_to_csv(self, accuracy_df):\n",
    "            # Start with summary1.csv\n",
    "            file_number = 0\n",
    "            while True:\n",
    "                filename = 'tune_log/'+'ml_summary' + str(file_number) + '.csv'\n",
    "                # Check if the file already exists\n",
    "                if not os.path.isfile(filename):\n",
    "                    break\n",
    "                file_number += 1\n",
    "            filename_model ='tune_log/'+ 'ml_model' + str(file_number) + '.h5'\n",
    "            network.model.save(filename_model)\n",
    "            accuracy_df.to_csv(filename, index=False)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tuning():\n",
    "    def __init__(self):\n",
    "        self.sequence_of_image_array = [5,6,8,9,10]\n",
    "        self.learning_rate_array = [0.00005,0.00003, 0.00004, 0.00001,0.0000008, 0.000006 ]\n",
    "        self.reshuffle_array=[False, True]\n",
    "        self.regularization_constant_array = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "        self.dense_neurons2_array = [8, 16, 32]\n",
    "        \n",
    "        self.sequence_of_image =  self.sequence_of_image_array[0]\n",
    "        self.learning_rate = self.learning_rate_array[1]\n",
    "        self.reshuffle =  self.reshuffle_array[0]\n",
    "        self.dropout1 = 0.5\n",
    "        self.dropout2 = 0.5\n",
    "        self.dropout3 = 0.5\n",
    "        self.dropout4 = 0.5\n",
    "        self.regularization_constant = 0.001\n",
    "        self.batch_size = 8\n",
    "        self.dense_neurons1 = 64\n",
    "        self.dense_neurons2 = 8\n",
    "        self.csv_id = 0\n",
    "        self.no_of_samples = 50\n",
    "        self.epochs = 50\n",
    "\n",
    "        \n",
    "    def start_training(self):\n",
    "        try:\n",
    "            manage_data.load_data(no_of_samples=self.no_of_samples)\n",
    "            manage_data.shuffle_file_paths()\n",
    "            manage_data.create_split_filepaths()\n",
    "            manage_data.create_split_datasets()\n",
    "            network.vgg_lstm()\n",
    "            network.train(manage_data.train_dataset, manage_data.val_dataset)\n",
    "        \n",
    "        # Ensure accuracy data is saved even if training is interrupted \n",
    "        finally:        \n",
    "            # Create a DataFrame from the accuracy history lists\n",
    "            accuracy_df = accuracy_history.create_accuracy_dataframe()\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            accuracy_history.save_to_csv(accuracy_df)\n",
    "            accuracy_history.reset_dict()                    \n",
    "    def Tune(self):\n",
    "        \n",
    "        for value in self.regularization_constant_array:\n",
    "            self.regularization_constant = value           \n",
    "            self.start_training()\n",
    "        self.regularization_constant = 0.001\n",
    "            \n",
    "        for value in self.learning_rate_array:\n",
    "            self.learning_rate = value           \n",
    "            self.start_training()\n",
    "        self.learning_rate = 0.00001\n",
    "        for value in self.sequence_of_image_array:\n",
    "            self.sequence_of_image = value           \n",
    "            self.start_training()\n",
    "        self.sequence_of_image = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subdirectories(directory):\n",
    "    try:\n",
    "        # Get the list of all entries in the directory\n",
    "        entries = os.listdir(directory)\n",
    "        \n",
    "        # Filter out and list only the directories\n",
    "        subdirs = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "        \n",
    "        return subdirs\n",
    "    except FileNotFoundError:\n",
    "        return f\"The directory '{directory}' does not exist.\"\n",
    "    except PermissionError:\n",
    "        return f\"Permission denied to access '{directory}'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_data = Manage_data()\n",
    "network = create_network()\n",
    "ml = ml_algorithm()\n",
    "manage_data= Manage_data()\n",
    "tune = tuning()\n",
    "accuracy_history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_filepaths (1063, 5)\n",
      "test_filepaths (153, 5)\n",
      "val_filepaths (303, 5)\n"
     ]
    }
   ],
   "source": [
    "manage_data.load_data(no_of_samples=10)\n",
    "manage_data.shuffle_file_paths()\n",
    "manage_data.create_split_filepaths()\n",
    "\n",
    "print('train_filepaths', manage_data.train_filepaths.shape)\n",
    "print('test_filepaths', manage_data.test_filepaths.shape)\n",
    "print('val_filepaths', manage_data.val_filepaths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1836', '0', '1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '101', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '102', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1029', '103', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '194', '1940', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '195', '1950', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '196', '1960', '1477', '1478', '1479', '148', '1480', '1481', '1482', '1483', '1484', '1485', '1486', '1487', '1488', '1489', '149', '1490', '1491', '1492', '1493', '1494', '1495', '1496', '1497', '1498', '1499', '15', '150', '1500', '1501', '1502', '1503', '639', '64', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '65', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '359', '36', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '37', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '38', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '39', '390', '391', '392', '393', '394', '395', '396', '397', '1030', '1053', '1079', '1105', '1130', '1157', '1183', '1204', '1227', '1254', '1288', '132', '1344', '1369', '140', '1427', '1452', '1476', '1504', '1531', '1555', '1576', '1607', '1635', '1669', '1694', '1726', '176', '1795', '1816', '1255', '1256', '1257', '1258', '1259', '126', '1260', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '127', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1277', '1278', '1279', '128', '1280', '1281', '1282', '1283', '1284', '1285', '1286', '1287', '1695', '1696', '1697', '1698', '1699', '17', '170', '1700', '1701', '1702', '1703', '1704', '1705', '1706', '1707', '1708', '1709', '171', '1710', '1711', '1712', '1713', '1714', '1715', '1716', '1717', '1718', '1719', '172', '1720', '1721', '1722', '1723', '1724', '1725', '847', '848', '849', '85', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '86', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '87', '870', '871', '872', '873', '874', '1106', '1107', '1108', '1109', '111', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '112', '1120', '1121', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '113', '513', '514', '515', '516', '517', '518', '519', '52', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '53', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '54', '1577', '1578', '1579', '158', '1580', '1581', '1582', '1583', '1584', '1585', '1586', '1587', '1588', '1589', '159', '1590', '1591', '1592', '1593', '1594', '1595', '1596', '1597', '1598', '1599', '16', '160', '1600', '1601', '1602', '1603', '1604', '1605', '1606', '1796', '1797', '1798', '1799', '18', '180', '1800', '1801', '1802', '1803', '1804', '1805', '1806', '1807', '1808', '1809', '181', '1810', '1811', '1812', '1813', '1814', '1815', '137', '1370', '1371', '1372', '1373', '1374', '1375', '1376', '1377', '1378', '1379', '138', '1380', '1381', '1382', '1383', '1385', '1386', '1387', '1388', '1389', '139', '1390', '1391', '1392', '1393', '1394', '1395', '1396', '1397', '1398', '1399', '14', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '26', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '27', '742', '743', '744', '745', '746', '747', '748', '749', '75', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '76', '760', '761', '762', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '567', '568', '569', '57', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '58', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '59', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '1636', '1637', '1638', '1639', '164', '1640', '1641', '1642', '1643', '1644', '1645', '1646', '1647', '1648', '1649', '165', '1650', '1651', '1652', '1653', '1654', '1655', '1656', '1657', '1658', '1659', '166', '1660', '1661', '1662', '1663', '1664', '1665', '1666', '1667', '1668', '1205', '1206', '1207', '1208', '1209', '121', '1210', '1211', '1212', '1213', '1214', '1215', '1216', '1217', '1218', '1219', '122', '1220', '1221', '1222', '1223', '1224', '1225', '1226', '908', '909', '91', '910', '911', '912', '913', '914', '915', '916', '917', '918', '919', '92', '920', '921', '922', '923', '924', '925', '926', '927', '928', '436', '437', '438', '439', '44', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '45', '450', '451', '452', '453', '454', '455', '456', '457', '458', '1884', '1885', '1886', '1887', '1888', '1889', '189', '1890', '1895', '1896', '1897', '1898', '1899', '19', '190', '1900', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2', '20', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '301', '302', '303', '304', '305', '306', '307', '308', '309', '31', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '32', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '1532', '1533', '1534', '1535', '1536', '1537', '1538', '1539', '154', '1540', '1541', '1542', '1543', '1544', '1545', '1546', '1547', '1548', '1549', '155', '1550', '1551', '1552', '1553', '1554', '1727', '1728', '1729', '173', '1730', '1731', '1732', '1733', '1734', '1735', '1736', '1737', '1738', '1739', '174', '1740', '1741', '1742', '1743', '1744', '1745', '1746', '1747', '1748', '1749', '175', '1750', '1751', '1752', '1753', '1754', '1755', '1756', '1757', '1758', '1759', '1320', '1321', '1322', '1323', '1324', '1325', '1326', '1327', '1328', '1329', '133', '1330', '1331', '1332', '1333', '1334', '1335', '1336', '1337', '1338', '1339', '134', '1340', '1341', '1342', '1343', '1428', '1429', '143', '1430', '1431', '1432', '1433', '1434', '1435', '1436', '1437', '1438', '1439', '144', '1440', '1441', '1445', '1446', '1447', '1448', '1449', '145', '1450', '1451', '785', '786', '787', '788', '789', '79', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '8', '80', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '81', '810', '686', '687', '688', '689', '69', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '7', '70', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '71', '710', '711', '712', '713', '714', '1158', '1159', '116', '1160', '1161', '1162', '1163', '1164', '1165', '1166', '1167', '1168', '1169', '117', '1170', '1171', '1172', '1173', '1174', '1175', '1176', '1177', '1178', '1179', '118', '1180', '1181', '1182', '959', '96', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '97', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '98', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '99', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1837', '1838', '1839', '184', '1840', '1841', '1842', '1843', '1844', '1845', '1846', '185', '1850', '1851', '1852', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '186', '1860', '1861', '1862', '1863', '60', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '61', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '62', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '63', '630', '631', '632', '633', '634', '635', '636', '637', '399', '4', '40', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '41', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '42', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '43', '430', '431', '432', '433', '434', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1038', '1039', '104', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '105', '1050', '1051', '1052', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '49', '490', '491', '492', '493', '494', '495', '496', '5', '50', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '51', '510', '511', '1760', '1761', '1762', '1763', '1764', '1765', '1766', '1767', '1768', '1769', '177', '1770', '1771', '1772', '1773', '1774', '1775', '1776', '1777', '1778', '1779', '178', '1780', '1781', '1782', '1783', '1784', '1785', '1786', '1787', '1788', '1789', '179', '1790', '1791', '1792', '1793', '1794', '1608', '1609', '161', '1610', '1611', '1612', '1613', '1614', '1615', '1616', '1617', '1618', '1619', '162', '1620', '1621', '1622', '1623', '1624', '1625', '1626', '1627', '1628', '1629', '163', '1630', '1631', '1632', '1633', '1634', '1400', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1409', '141', '1410', '1411', '1412', '1413', '1414', '1415', '1416', '1417', '1418', '1419', '142', '1420', '1421', '1422', '1423', '1424', '1425', '1426', '1289', '129', '1290', '1291', '1292', '1293', '1294', '1295', '1296', '1297', '1298', '1299', '13', '130', '1300', '1301', '1302', '1303', '1304', '1305', '1306', '1307', '1308', '1309', '131', '1310', '1311', '1312', '1313', '1314', '1315', '1316', '1317', '1318', '1319', '1505', '1506', '1507', '1508', '1509', '151', '1510', '1511', '1512', '1513', '1514', '1515', '1516', '1517', '1518', '1519', '152', '1520', '1521', '1522', '1523', '1524', '1525', '1526', '1527', '1528', '1529', '153', '1530', '1864', '1883', '191', '193', '1961', '199', '218', '245', '270', '300', '33', '358', '398', '435', '459', '48', '512', '540', '566', '6', '638', '66', '685', '715', '741', '763', '784', '811', '846', '875', '907', '929', '958', '716', '717', '718', '719', '72', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '73', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '74', '740', '167', '1670', '1671', '1672', '1673', '1674', '1675', '1676', '1677', '1678', '1679', '168', '1680', '1681', '1682', '1683', '1684', '1685', '1686', '1687', '1688', '1689', '169', '1690', '1691', '1692', '1693', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '198', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '876', '877', '878', '879', '88', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '89', '890', '891', '892', '893', '894', '895', '896', '897', '898', '899', '9', '90', '900', '901', '902', '903', '904', '905', '906', '812', '813', '814', '815', '816', '817', '818', '819', '82', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '83', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '84', '840', '841', '842', '843', '844', '845', '271', '272', '273', '274', '275', '276', '277', '278', '279', '28', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '29', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '3', '30', '219', '22', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '24', '240', '241', '242', '243', '244', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '114', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '115', '1150', '1151', '1152', '1153', '1154', '1155', '1156', '1228', '1229', '123', '1230', '1231', '1232', '1233', '1234', '1235', '1236', '1237', '1238', '1239', '124', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1248', '1249', '125', '1250', '1251', '1252', '1253', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '34', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '35', '350', '351', '352', '353', '354', '355', '356', '357', '541', '542', '543', '544', '545', '546', '547', '548', '549', '55', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '56', '560', '561', '562', '563', '564', '565', '1080', '1081', '1082', '1083', '1084', '1085', '1090', '1091', '1092', '1093', '1094', '1095', '1096', '1097', '1098', '1099', '11', '110', '1100', '1101', '1102', '1103', '1104', '1345', '1346', '1347', '1348', '1349', '135', '1350', '1351', '1352', '1353', '1354', '1355', '1356', '1357', '1358', '1359', '136', '1360', '1361', '1362', '1363', '1364', '1365', '1366', '1367', '1368', '1184', '1185', '1186', '1187', '1188', '1189', '119', '1190', '1191', '1192', '1193', '1194', '1195', '1196', '1197', '1198', '1199', '12', '120', '1200', '1201', '1202', '1203', '93', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '94', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '95', '950', '951', '952', '953', '954', '955', '956', '957', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '146', '1460', '1461', '1462', '1463', '1464', '1465', '1466', '1467', '1468', '1469', '147', '1470', '1471', '1472', '1473', '1474', '1475', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '67', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '68', '680', '681', '682', '683', '684', '1556', '1557', '1558', '1559', '156', '1560', '1561', '1562', '1563', '1564', '1565', '1566', '1567', '1568', '1569', '157', '1570', '1571', '1572', '1573', '1574', '1575', '1910', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1865', '1866', '1867', '1868', '1869', '187', '1870', '1871', '1872', '1873', '1874', '1875', '1876', '1877', '1878', '1879', '188', '1880', '1881', '1882', '1817', '1818', '1819', '182', '1820', '1821', '1822', '1823', '1824', '1825', '1826', '1827', '1828', '1829', '183', '1830', '1831', '1832', '1833', '1834', '1835', '46', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '47', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '764', '765', '766', '767', '768', '769', '77', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '78', '780', '781', '782', '783']\n"
     ]
    }
   ],
   "source": [
    "subdirectories = list_subdirectories(manage_data.data_dir)\n",
    "print(subdirectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: (8, 5, 224, 224, 3)\n",
      "Labels batch shape: (8,)\n",
      "Images batch shape: (8, 5, 224, 224, 3)\n",
      "Labels batch shape: (8,)\n",
      "Images batch shape: (8, 5, 224, 224, 3)\n",
      "Labels batch shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "manage_data.create_split_datasets()\n",
    "for batch in manage_data.train_dataset.take(1):  # Take one batch to print its shape\n",
    "    images_batch, labels_batch = batch\n",
    "    print(\"Images batch shape:\", images_batch.shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.shape)\n",
    "for batch in manage_data.test_dataset.take(1):  # Take one batch to print its shape\n",
    "    images_batch, labels_batch = batch\n",
    "    print(\"Images batch shape:\", images_batch.shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.shape)\n",
    "for batch in manage_data.val_dataset.take(1):  # Take one batch to print its shape\n",
    "    images_batch, labels_batch = batch\n",
    "    print(\"Images batch shape:\", images_batch.shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sequence in manage_data.train_dataset:\n",
    "#     input_data, labels = sequence  # Unpack the tuple\n",
    "#     # for sequence2 in input_data:\n",
    "#     #     print('sequence=', sequence2.shape)\n",
    "#     print(\"Images batch shape:\", input_data.shape)\n",
    "#     print(\"Labels batch shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=100 must be between 0 and min(n_samples, n_features)=5 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# train_dataset - (None, [8, 5, 224, 224, 3],[8])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# above one is a simplified version of a more complex tensor\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence, label \u001b[38;5;129;01min\u001b[39;00m manage_data\u001b[38;5;241m.\u001b[39mtrain_dataset:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# sequence- (8, 5, 224, 224, 3)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# label - (8)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     processed_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     train_sequences\u001b[38;5;241m.\u001b[39mappend(processed_sequence)\n\u001b[1;32m     13\u001b[0m     train_labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "Cell \u001b[0;32mIn[255], line 30\u001b[0m, in \u001b[0;36mml_algorithm.process_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m,batch):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_hog_features(sequence) \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "Cell \u001b[0;32mIn[255], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m,batch):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_hog_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "Cell \u001b[0;32mIn[255], line 24\u001b[0m, in \u001b[0;36mml_algorithm.extract_hog_features\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     21\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(features)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# features  (5, 26244)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Reduce features using PCA\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduced features\u001b[39m\u001b[38;5;124m'\u001b[39m, reduced_features)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(reduced_features)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:510\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:524\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features))\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Center data\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=100 must be between 0 and min(n_samples, n_features)=5 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "train_sequences = []\n",
    "train_labels = []\n",
    "val_sequences = []\n",
    "val_labels = []\n",
    "\n",
    "# train_dataset - (None, [8, 5, 224, 224, 3],[8])\n",
    "# above one is a simplified version of a more complex tensor\n",
    "for sequence, label in manage_data.train_dataset:\n",
    "    # sequence- (8, 5, 224, 224, 3)\n",
    "    # label - (8)\n",
    "    processed_sequence = ml.process_batch(sequence)\n",
    "    train_sequences.append(processed_sequence)\n",
    "    train_labels.append(label)\n",
    "\n",
    "for sequence, label in manage_data.val_dataset:\n",
    "    processed_sequence = ml.process_batch(sequence)\n",
    "    val_sequences.append(processed_sequence)\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (133,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_sequences\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_sequences=\u001b[39m\u001b[38;5;124m'\u001b[39m, train_sequences\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_labels=\u001b[39m\u001b[38;5;124m'\u001b[39m, train_labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (133,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "train_sequences=np.array(train_sequences)\n",
    "print('train_sequences=', train_sequences.shape)\n",
    "print('train_labels=', train_labels.shape)\n",
    "\n",
    "\n",
    "ml.train(train_sequences, train_labels, val_sequences, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the model from the .h5 file\n",
    "# model = load_model('model_vgg.h5')\n",
    "# model.evaluate(manage_data2.test_dataset)\n",
    "\n",
    "# # Print the model summary to confirm it was loaded correctly\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
