{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "# Set the environment variable to use only the GPU with ID 1 (GTX 1080 Ti)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is demo code on to check how preprocess the images for vgg network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Manage_data():\n",
    "    def __init__(self):\n",
    "        data_dir='/home/rag-tt/workspace/'\n",
    "        self.train_data_dir = os.path.join(data_dir,'train_data')\n",
    "        self.test_data_dir = os.path.join(data_dir,'test_data')\n",
    "        self.hog_features_dir = os.path.join(data_dir,'hog_features')\n",
    "        self.train_features_dir = os.path.join(self.hog_features_dir,'train_features')\n",
    "        self.test_features_dir = os.path.join(self.hog_features_dir,'test_features')\n",
    "        self.data_dir= pathlib.Path(data_dir)\n",
    "        \n",
    "    def count_subdirectories(self,directory):\n",
    "        try:\n",
    "            # List all entries in the directory\n",
    "            entries = os.listdir(directory)\n",
    "            \n",
    "            # Filter out the subdirectories\n",
    "            subdirectories = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "            \n",
    "            # Count the subdirectories\n",
    "            num_subdirectories = len(subdirectories)\n",
    "            \n",
    "            print(f\"The directory '{directory}' contains {num_subdirectories} subdirectories.\")\n",
    "            return num_subdirectories\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"The directory '{directory}' does not exist.\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to access '{directory}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    def find_no_of_images(self, data_dir,obj_id):\n",
    "        image_dir = os.path.join(data_dir, str(obj_id))\n",
    "        image_dir= pathlib.Path(image_dir)\n",
    "        no_of_images= len(list(image_dir.glob('*.jpg')))\n",
    "        return no_of_images\n",
    "    \n",
    "    def trim_data(self, label, image_paths, csv_path):\n",
    "        # Ensure label and image_paths are numpy arrays\n",
    "        label = np.array(label)\n",
    "        image_paths = np.array(image_paths)\n",
    "        \n",
    "        # Count the total number of zeroes in label\n",
    "        total_zeroes = np.sum(label == 0)\n",
    "        \n",
    "        # Determine the number of zeroes to remove\n",
    "        zeroes_to_remove = max(0, total_zeroes - tune.no_of_nonslip_data)\n",
    "        \n",
    "        # Indices of zero elements\n",
    "        zero_indices = np.where(label == 0)[0]\n",
    "        \n",
    "        # Indices to keep (last self.no_of_nonslip_data zeroes and all ones)\n",
    "        indices_to_keep = np.concatenate((zero_indices[-tune.no_of_nonslip_data:], np.where(label != 0)[0]))\n",
    "        indices_to_keep = np.unique(indices_to_keep)\n",
    "        indices_to_keep = np.sort(indices_to_keep)\n",
    "        \n",
    "        # Create the resulting label array\n",
    "        label_with_few_zeroes = label[indices_to_keep]\n",
    "        \n",
    "        # Remove the same number of elements from the start of image_paths\n",
    "        paths_with_few_zeroes = image_paths[zeroes_to_remove:]\n",
    "        \n",
    "        \n",
    "        min_tranistion_trim_value = tune.min_trim_value\n",
    "        max_tranistion_trim_value = tune.slip_instant_labels\n",
    "        trimmed_labels = []\n",
    "        trimmed_paths = []\n",
    "        slip_values = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=2, dtype=None, encoding=None)\n",
    "        i = 0\n",
    "        for slip_value in slip_values:\n",
    "            if slip_value < tune.max_labels:\n",
    "                if max_tranistion_trim_value < slip_value or slip_value < min_tranistion_trim_value:\n",
    "                    trimmed_labels.append(label_with_few_zeroes[i])\n",
    "                    trimmed_paths.append(paths_with_few_zeroes[i])\n",
    "            i += 1\n",
    "        trimmed_labels = np.array(trimmed_labels)\n",
    "        trimmed_paths = np.array(trimmed_paths)\n",
    "        # print(slip_values)\n",
    "        # print('label_with_few_zeroes =', label_with_few_zeroes)\n",
    "        # print('paths_with_few_zeroes=', paths_with_few_zeroes.shape)\n",
    "        # print('trimmed_labels=', trimmed_labels)\n",
    "        # print('trimed_paths=', trimmed_paths.shape)\n",
    "        return trimmed_labels, trimmed_paths\n",
    "    \n",
    "    def check_pattern(self,label):\n",
    "        # Ensure arr is a numpy array\n",
    "        label = np.array(label)\n",
    "        \n",
    "        # Find the first occurrence of 1\n",
    "        first_one_index = np.argmax(label == 1)\n",
    "        \n",
    "        if np.all(label == 0):  # If there's no 1 in the array, ensure all are 0\n",
    "            return\n",
    "        \n",
    "        # Check if there's no 1 in the array\n",
    "        if np.max(label) == 0:\n",
    "            assert np.all(label == 0), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "            return\n",
    "        \n",
    "        # Assert all elements before first_one_index are 0\n",
    "        assert np.all(label[:first_one_index] == 0), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "        \n",
    "        # Assert all elements from first_one_index to the end are 1\n",
    "        assert np.all(label[first_one_index:] == 1), \"Array does not follow the pattern: continuous zeroes followed by continuous ones\"\n",
    "    \n",
    "    def create_slip_instant_labels(self, csv_path):\n",
    "        label = []\n",
    "        slip_values = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=2, dtype=None, encoding=None)\n",
    "        for slip_value in slip_values:\n",
    "            if slip_value < tune.slip_instant_labels:\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(1)\n",
    "        return label\n",
    " \n",
    "    def duplicate_n_balance_data(self, labels, image_paths):\n",
    "        # Convert labels to numpy array for easier manipulation\n",
    "        labels = np.array(labels)\n",
    "        image_paths = np.array(image_paths)\n",
    "\n",
    "        # Get indices of each class\n",
    "        class_0_indices = np.where(labels == 0)[0]\n",
    "        class_1_indices = np.where(labels == 1)[0]\n",
    "            # Check if either class is empty\n",
    "        if len(class_0_indices) == 0 or len(class_1_indices) == 0:\n",
    "            # print(f\"Skipping balancing for {labels} as one of the classes is missing\")\n",
    "            return labels, image_paths\n",
    "    \n",
    "        # Calculate the difference in the number of samples\n",
    "        diff = len(class_0_indices) - len(class_1_indices)\n",
    "\n",
    "        if diff > 0:  # More 0s than 1s\n",
    "            # Randomly duplicate class 1 samples to balance the dataset\n",
    "            additional_indices = np.random.choice(class_1_indices, size=diff, replace=True)\n",
    "            labels = np.concatenate([labels, labels[additional_indices]])\n",
    "            image_paths = np.concatenate([image_paths, image_paths[additional_indices]])\n",
    "        elif diff < 0:  # More 1s than 0s\n",
    "            # Randomly duplicate class 0 samples to balance the dataset\n",
    "            additional_indices = np.random.choice(class_0_indices, size=-diff, replace=True)\n",
    "            labels = np.concatenate([labels, labels[additional_indices]])\n",
    "            image_paths = np.concatenate([image_paths, image_paths[additional_indices]])\n",
    "        # # Shuffle the dataset to mix the duplicated samples\n",
    "        # shuffle_indices = np.arange(len(labels))\n",
    "        # np.random.shuffle(shuffle_indices)\n",
    "        # labels = labels[shuffle_indices]\n",
    "        # image_paths = image_paths[shuffle_indices]\n",
    "\n",
    "        return labels, image_paths           \n",
    "            \n",
    "    def balance_n_separate_data(self, labels, image_paths):\n",
    "        # Convert labels to numpy array for easier manipulation\n",
    "        labels = np.array(labels)\n",
    "        image_paths = np.array(image_paths)\n",
    "\n",
    "        # Get indices of each class\n",
    "        class_0_indices = np.where(labels == 0)[0]\n",
    "        class_1_indices = np.where(labels == 1)[0]\n",
    "\n",
    "        labels_0=[]\n",
    "        labels_1=[]\n",
    "        image_paths_0=[]\n",
    "        image_paths_1=[]\n",
    "        \n",
    "        # Check if either class is empty\n",
    "        if len(class_0_indices) == 0 or len(class_1_indices) == 0:\n",
    "            print(f\"Skipping balancing for {labels} as one of the classes is missing\")\n",
    "            return labels_0,labels_1,image_paths_0,image_paths_1\n",
    "\n",
    "\n",
    "        #separate_data\n",
    "        for i in class_0_indices:\n",
    "            labels_0.append(labels[i])\n",
    "            image_paths_0.append(image_paths[i])\n",
    "        for i in class_1_indices:\n",
    "            labels_1.append(labels[i])\n",
    "            image_paths_1.append(image_paths[i])   \n",
    "        \n",
    "        # print('labels_0',labels_0)\n",
    "        # print('image_paths_0',image_paths_0)\n",
    "        # print('labels_1',labels_1)\n",
    "        # print('image_paths_1',image_paths_1)\n",
    "                 \n",
    "        # Calculate the difference in the number of samples\n",
    "        diff = len(class_0_indices) - len(class_1_indices)\n",
    "\n",
    "        if diff < 0:  # More 1s than 0s\n",
    "            # Randomly duplicate class 0 samples to balance the dataset\n",
    "            additional_indices = np.random.choice(class_0_indices, size=-diff, replace=True)\n",
    "            labels_0 = np.concatenate([labels_0, labels[additional_indices]])\n",
    "            image_paths_0 = np.concatenate([image_paths_0, image_paths[additional_indices]])\n",
    "        \n",
    "        # print('labels_0',labels_0)\n",
    "        # print('image_paths_0',image_paths_0)\n",
    "        # print('labels_1',labels_1)\n",
    "        # print('image_paths_1',image_paths_1)\n",
    "        \n",
    "        return labels_0, image_paths_0, labels_1, image_paths_1  \n",
    "    \n",
    "    def save_hog_features_and_labels(self, features, labels, obj_id, feature_dir):\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "        labels_file = os.path.join(feature_dir, f'labels_{obj_id}.npy')\n",
    "        features_file = os.path.join(feature_dir, f'features_{obj_id}.npy')\n",
    "        \n",
    "        np.save(labels_file, labels)\n",
    "        np.save(features_file, features, allow_pickle=True)\n",
    "        \n",
    "    def process_features(self,data_dir, no_of_samples, feature_dir):\n",
    "        file_paths = []\n",
    "        features = []\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj_id in range(no_of_samples):\n",
    "            image_paths = []\n",
    "            no_of_images = self.find_no_of_images(data_dir,obj_id)\n",
    "            csv_path = os.path.join(data_dir, str(obj_id),'slip_log.csv')\n",
    "            if no_of_images < 40 or not os.path.exists(csv_path):\n",
    "                continue\n",
    "            label = self.create_slip_instant_labels(csv_path)\n",
    "            \n",
    "            label2 = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=1, dtype=None, encoding=None)\n",
    "            \n",
    "            for img_id in range(no_of_images):\n",
    "                image_path = os.path.join(data_dir, str(obj_id),'depth'+ str(img_id)+ '.png')\n",
    "                image_paths.append(image_path)\n",
    "            self.check_pattern(label)\n",
    "            \n",
    "            label, image_paths = self.trim_data(label,image_paths,csv_path)  \n",
    "\n",
    "            #check if the labels are correct\n",
    "            np_label = np.array(label)  \n",
    "            if np.all(np_label==1):\n",
    "                image_paths = []\n",
    "                continue \n",
    "            \n",
    "            class_0_indices = np.where(label == 0)[0]\n",
    "            class_1_indices = np.where(label == 1)[0]\n",
    "            \n",
    "            if len(class_0_indices) > len(class_1_indices):\n",
    "                image_paths = []\n",
    "                continue\n",
    "            \n",
    "            '''            \n",
    "            example values of label and image_paths pair             \n",
    "            label size = 83, imagepaths_size = 83 \n",
    "            zeroes = 41, ones = 42\n",
    "            img40 == 0, img41 == 1\n",
    "            '''\n",
    "            label_0, image_paths_0, label_1, image_paths_1 = self.balance_n_separate_data(label,image_paths) \n",
    "\n",
    "            label_0, features_0, label_1, features_1 = ml.create_ml_features(label_0, image_paths_0, label_1, image_paths_1)     \n",
    "            \n",
    "            labels.append(label_0)\n",
    "            labels.append(label_1)\n",
    "            features.append(features_0)\n",
    "            features.append(features_1)\n",
    "            self.save_hog_features_and_labels(features, label, obj_id, feature_dir)\n",
    "            labels = []\n",
    "            features = []\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import data, exposure\n",
    "\n",
    "class ml_algorithm():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier(n_estimators=100)\n",
    "        self.pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "    \n",
    "    def find_hog_features_using_paths(self, image_paths):\n",
    "        features = []\n",
    "        for image_path in image_paths:\n",
    "            image = cv2.imread(image_path)\n",
    "            gray_img = rgb2gray(image)\n",
    "            hog_features, hog_image = hog(gray_img, pixels_per_cell=(64, 64), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "            # reduced_features = self.pca.fit_transform(hog_features)\n",
    "            features.append(hog_features)\n",
    "        features = np.array(features)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_ml_features(self,label_0, image_paths_0, label_1, image_paths_1):\n",
    "        hog_features_0 = self.find_hog_features_using_paths(image_paths_0)\n",
    "        hog_features_1 = self.find_hog_features_using_paths(image_paths_1)\n",
    "        print(hog_features_0.shape, hog_features_1.shape)\n",
    "        \n",
    "        \n",
    "        # club hog features together seperately and then join together in one dataset\n",
    "        clubbed_hog_features_0 = []\n",
    "        for i in range(0, len(hog_features_0 ) - (tune.img_sequence_window_size-1), tune.stride):  # Ensuring sequences of 5 images\n",
    "            row = hog_features_0[i:i+tune.img_sequence_window_size]\n",
    "            clubbed_hog_features_0.append(row)\n",
    "        \n",
    "        clubbed_hog_features_1 = []    \n",
    "        for i in range(0, len(hog_features_1) - (tune.img_sequence_window_size-1), tune.stride):  # Ensuring sequences of 5 images\n",
    "            row = hog_features_1[i:i+tune.img_sequence_window_size]\n",
    "            clubbed_hog_features_1.append(row)\n",
    "                        \n",
    "        hog_features_0 = []\n",
    "        hog_features_1 = []\n",
    "        label_0 = np.array(label_0[(tune.img_sequence_window_size-1):])\n",
    "        label_0 =  label_0[::tune.stride]\n",
    "        label_1 = np.array(label_1[(tune.img_sequence_window_size-1):])\n",
    "        label_1 =  label_1[::tune.stride]\n",
    "        \n",
    "        return label_0, clubbed_hog_features_0, label_1, clubbed_hog_features_1\n",
    "        \n",
    "    \n",
    "    def extract_hog_features(self,images):\n",
    "        features = []\n",
    "        # images (5,255,255,3)\n",
    "        for img in images:\n",
    "            gray_img = rgb2gray(img)\n",
    "            hog_features = hog(gray_img, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "            # reduced_features = self.pca.fit_transform(hog_features)\n",
    "            features.append(hog_features )\n",
    "            \n",
    "        features = np.array(features)\n",
    "        # features  (5, 26244)\n",
    "        \n",
    "        print('reduced features', features)\n",
    "        return np.array(features)\n",
    "\n",
    "    # Example function to process a batch of sequences\n",
    "    def process_batch(self,batch):\n",
    "        return np.array([self.extract_hog_features(sequence) for sequence in batch])\n",
    "    \n",
    "    def train(self, train_sequences, train_labels, val_sequences, val_labels):\n",
    "        # Flatten the sequence of HOG features\n",
    "        train_features = [sequence.flatten() for sequence in train_sequences]\n",
    "        val_features = [sequence.flatten() for sequence in val_sequences]\n",
    "        \n",
    "        # Train the Random Forest model\n",
    "        self.model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        val_predictions = self.model.predict(val_features)\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset_dict()\n",
    "             \n",
    "    def reset_dict(self):\n",
    "        self.epoch_count = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.img_sequence_window_size = []\n",
    "        self.stride = []\n",
    "        self.learning_rate = []\n",
    "        self.reshuffle =  []\n",
    "        self.dropout1 = []\n",
    "        self.dropout2 = []\n",
    "        self.dropout3 = []\n",
    "        self.dropout4 = []\n",
    "        self.regularization_constant = []\n",
    "        self.batch_size = []\n",
    "        self.dense_neurons1 =[]\n",
    "        self.dense_neurons2 =[]\n",
    "        self.no_of_samples = []\n",
    "        self.epochs  = []\n",
    "        self.vgg_layers = []\n",
    "        self.other_param = []\n",
    "        self.no_of_nonslip_data = []\n",
    "        self.slip_instant_labels = [] \n",
    "        self.max_labels = []\n",
    "        self.tp = []\n",
    "        self.tn = []\n",
    "        self.fp = []\n",
    "        self.fn = []\n",
    "        self.tpr = []\n",
    "        self.tnr = []\n",
    "        self.fnr = []\n",
    "        self.f1 = []\n",
    "        self.validation_data = None  \n",
    "       \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        if hasattr(self.model, 'validation_data') and self.model.validation_data:\n",
    "            self.validation_data = (self.model.validation_data[0], self.model.validation_data[1])\n",
    "        else:\n",
    "            print(\"Validation data is not available at the start of training.\")    \n",
    "     \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {'accuracy': 0, 'val_accuracy': 0,'true_positives':0,'true_negatives':0,'false_positives':0,'false_negatives':0 }\n",
    "        \n",
    "\n",
    "        tp = logs.get('true_positives')\n",
    "        tn = logs.get('true_negatives')\n",
    "        fp = logs.get('false_positives')\n",
    "        fn = logs.get('false_negatives')\n",
    "\n",
    "\n",
    "        # Compute TPR, TNR, and F1 score\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        f1 =  (tp + tn) /(tn + fp + fn + tp) if (tn + fp + fn + tp) > 0 else 0\n",
    "        # print('tp= ',tp,'tn= ',tn,'fp= ',fp,'fn= ',fn)\n",
    "        # print('tpr= ',tpr,'fnr= ',fnr,'f1= ',f1)\n",
    "\n",
    "        self.epoch_count.append(epoch + 1)\n",
    "        self.train_accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        self.img_sequence_window_size.append(tune.img_sequence_window_size)\n",
    "        self.stride.append(tune.stride)\n",
    "        self.learning_rate.append(tune.learning_rate)\n",
    "        self.reshuffle.append(tune.reshuffle)\n",
    "        self.dropout1.append(tune.dropout1)\n",
    "        self.dropout2.append(tune.dropout2)\n",
    "        self.dropout3.append(tune.dropout3)\n",
    "        self.dropout4.append(tune.dropout4)\n",
    "        self.regularization_constant.append(tune.regularization_constant)\n",
    "        self.batch_size.append(tune.batch_size)\n",
    "        self.dense_neurons1.append(tune.dense_neurons1)\n",
    "        self.dense_neurons2.append(tune.dense_neurons2)\n",
    "        self.no_of_samples.append(tune.no_of_samples)\n",
    "        self.epochs.append(tune.epochs )\n",
    "        self.vgg_layers.append(tune.vgg_layers)\n",
    "        self.other_param.append(tune.other_param)\n",
    "        self.no_of_nonslip_data.append(tune.no_of_nonslip_data)\n",
    "        self.slip_instant_labels.append(tune.slip_instant_labels)\n",
    "        self.max_labels.append(tune.max_labels)\n",
    "        self.tp.append(tp)\n",
    "        self.tn.append(tn)\n",
    "        self.fp.append(fp)\n",
    "        self.fn.append(fn)\n",
    "        self.tpr.append(tpr)\n",
    "        self.fnr.append(fnr)\n",
    "        self.f1.append(f1)\n",
    "\n",
    "        \n",
    "    def create_accuracy_dataframe(self):\n",
    "        accuracy_df = pd.DataFrame({\n",
    "            'Epoch': self.epoch_count,\n",
    "            'Train_Accuracy': self.train_accuracy,\n",
    "            'Val_Accuracy': self.val_accuracy,\n",
    "            'img_sequence_window_size': self.img_sequence_window_size,\n",
    "            'stride':self.stride,\n",
    "            'Learning_Rate': self.learning_rate,\n",
    "            'Reshuffle': self.reshuffle,\n",
    "            'Dropout1': self.dropout1,\n",
    "            'Dropout2': self.dropout2,\n",
    "            'Dropout3': self.dropout3,\n",
    "            'Dropout4': self.dropout4,\n",
    "            'Regularization_Constant': self.regularization_constant,\n",
    "            'Batch_Size': self.batch_size,\n",
    "            'dense_neurons1': self.dense_neurons1,\n",
    "            'dense_neurons2': self.dense_neurons2,\n",
    "            'no_of_samples':self.no_of_samples,\n",
    "            'epochs':self.epochs, \n",
    "            'vgg_layers':self.vgg_layers,\n",
    "            'other_param':self.other_param,\n",
    "            'no_of_nonslip_data':self.no_of_nonslip_data,\n",
    "            'slip_instant_labels':self.slip_instant_labels,\n",
    "            'max_labels':self.max_labels,\n",
    "            'tp':self.tp,\n",
    "            'tn':self.tn,\n",
    "            'fp':self.fp,\n",
    "            'fn':self.fn,\n",
    "            'tpr':self.tpr,\n",
    "            'fnr':self.fnr,\n",
    "            'f1':self.f1\n",
    "        })\n",
    "        return accuracy_df    \n",
    "    def save_to_csv(self, accuracy_df):\n",
    "            # Start with summary1.csv\n",
    "            file_number = 0\n",
    "            while True:\n",
    "                filename = 'tune_log/'+'summary' + str(file_number) + '.csv'\n",
    "                # Check if the file already exists\n",
    "                if not os.path.isfile(filename):\n",
    "                    break\n",
    "                file_number += 1\n",
    "            filename_model ='tune_log/'+ 'model' + str(file_number) + '.h5'\n",
    "            network.model.save(filename_model)\n",
    "            accuracy_df.to_csv(filename, index=False)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tuning():\n",
    "    def __init__(self):\n",
    "        self.img_sequence_window_size_array = [8, 10, 12]\n",
    "        self.learning_rate_array = [0.00003, 0.00001]\n",
    "        self.reshuffle_array=[False, True]\n",
    "        self.regularization_constant_array = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "        self.dense_neurons2_array = [8, 16, 32]\n",
    "        self.vgg_layers_array= [7,11,15,19]\n",
    "        self.slip_instant_labels_array = [0.0001,0.0005, 0.001, 0.003, 0.005]\n",
    "        \n",
    "        self.img_sequence_window_size =  self.img_sequence_window_size_array[0]\n",
    "        self.stride = 1 \n",
    "        self.learning_rate = self.learning_rate_array[0]\n",
    "        self.reshuffle =  self.reshuffle_array[0]\n",
    "        self.dropout1 = 0.5\n",
    "        self.dropout2 = 0.5\n",
    "        self.dropout3 = 0.5\n",
    "        self.dropout4 = 0.5\n",
    "        self.regularization_constant = 0.001\n",
    "        self.batch_size = 4\n",
    "        self.dense_neurons1 = 64\n",
    "        self.dense_neurons2 = 8\n",
    "        self.csv_id = 0\n",
    "        self.no_of_samples = 50\n",
    "        self.epochs = 40\n",
    "        self.vgg_layers = 19\n",
    "        self.other_param='additional cnn + global average'\n",
    "        self.no_of_nonslip_data = 2000\n",
    "        self.slip_instant_labels = 0.0003\n",
    "        self.max_labels = 0.02\n",
    "        self.min_trim_value = 0.000007\n",
    "    \n",
    "    def define_dataset(self,no_of_train_samples=1000000, no_of_test_samples=1000000):\n",
    "\n",
    "            train_data_dir = manage_data.train_data_dir\n",
    "            test_data_dir = manage_data.test_data_dir\n",
    "            \n",
    "            train_data_qty = manage_data.count_subdirectories(train_data_dir)\n",
    "            test_data_qty = manage_data.count_subdirectories(test_data_dir)\n",
    "            if no_of_train_samples > train_data_qty:\n",
    "                no_of_train_samples = train_data_qty\n",
    "                self.no_of_samples = train_data_qty\n",
    "            if no_of_test_samples > test_data_qty:\n",
    "                no_of_test_samples = test_data_qty\n",
    "            \n",
    "            train_labels, train_file_paths = manage_data.load_and_club_data(data_dir = train_data_dir, no_of_samples=no_of_train_samples)\n",
    "            test_labels, test_file_paths = manage_data.load_and_club_data(data_dir = test_data_dir, no_of_samples=no_of_test_samples)\n",
    "            \n",
    "            train_labels, train_file_paths = manage_data.shuffle_file_paths(train_labels, train_file_paths)\n",
    "            train_labels = np.array(train_labels)\n",
    "            test_labels = np.array(test_labels)\n",
    "            print('train_labels_type',train_labels.dtype)\n",
    "            print('test labels dtype',test_labels.dtype )\n",
    "            \n",
    "            train_dataset = manage_data.create_dataset(train_labels, train_file_paths )\n",
    "            test_dataset = manage_data.create_dataset(test_labels, test_file_paths) \n",
    "            return train_dataset, test_dataset \n",
    "                        \n",
    "    def start_training(self):\n",
    "        try:\n",
    "            train_dataset, test_dataset = self.define_dataset()\n",
    "            network.vgg_lstm()\n",
    "            \n",
    "            #print the tuning parametrs before training\n",
    "            accuracy_history.on_epoch_end(0)\n",
    "            df = accuracy_history.create_accuracy_dataframe()\n",
    "            # Transpose the DataFrame\n",
    "            df_transposed = df.transpose()\n",
    "            print(df_transposed)\n",
    "            network.train(train_dataset, test_dataset)\n",
    "        \n",
    "        # Ensure accuracy data is saved even if training is interrupted \n",
    "        finally:        \n",
    "            # Create a DataFrame from the accuracy history lists\n",
    "            accuracy_df = accuracy_history.create_accuracy_dataframe()\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            accuracy_history.save_to_csv(accuracy_df)\n",
    "            accuracy_history.reset_dict()             \n",
    "                   \n",
    "    def Tune(self):\n",
    "        # for value in self.vgg_layers_array:\n",
    "        #     self.vgg_layers = value         \n",
    "        #     self.start_training()\n",
    "        # self.vgg_layers= 19\n",
    "        \n",
    "        for value in self.learning_rate_array:\n",
    "            self.learning_rate = value           \n",
    "            self.start_training()\n",
    "        self.learning_rate = 0.00003\n",
    "        \n",
    "def true_positives(y_true, y_pred):\n",
    "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, 'float'), axis=0)\n",
    "    return tp\n",
    "\n",
    "def true_negatives(y_true, y_pred):\n",
    "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
    "    tn = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n",
    "    return tn\n",
    "\n",
    "def false_positives(y_true, y_pred):\n",
    "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "    return fp\n",
    "\n",
    "def false_negatives(y_true, y_pred):\n",
    "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subdirectories(directory):\n",
    "    try:\n",
    "        # Get the list of all entries in the directory\n",
    "        entries = os.listdir(directory)\n",
    "        \n",
    "        # Filter out and list only the directories\n",
    "        subdirs = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "        \n",
    "        return subdirs\n",
    "    except FileNotFoundError:\n",
    "        return f\"The directory '{directory}' does not exist.\"\n",
    "    except PermissionError:\n",
    "        return f\"Permission denied to access '{directory}'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_data = Manage_data()\n",
    "ml = ml_algorithm()\n",
    "tune = tuning()\n",
    "accuracy_history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory '/home/rag-tt/workspace/train_data' contains 10841 subdirectories.\n",
      "The directory '/home/rag-tt/workspace/test_data' contains 1041 subdirectories.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_dir = manage_data.train_data_dir\n",
    "test_data_dir = manage_data.test_data_dir\n",
    "\n",
    "train_data_qty = manage_data.count_subdirectories(train_data_dir)\n",
    "test_data_qty = manage_data.count_subdirectories(test_data_dir)\n",
    "\n",
    "train_features_dir =  manage_data.train_features_dir\n",
    "test_features_dir =  manage_data.test_features_dir\n",
    "\n",
    "# train_labels, train_file_paths = manage_data.load_and_club_data(data_dir = train_data_dir, no_of_samples=tune.no_of_samples)\n",
    "# test_labels, test_file_paths = manage_data.load_and_club_data(data_dir = test_data_dir, no_of_samples=test_data_qty)\n",
    "\n",
    "# train_labels, train_file_paths = manage_data.shuffle_file_paths(train_labels, train_file_paths)\n",
    "\n",
    "# train_dataset = manage_data.create_dataset(train_labels, train_file_paths )\n",
    "# test_dataset = manage_data.create_dataset(test_labels, test_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 1944) (69, 1944)\n",
      "(71, 1944) (71, 1944)\n",
      "(70, 1944) (70, 1944)\n",
      "(69, 1944) (69, 1944)\n",
      "(71, 1944) (71, 1944)\n",
      "(70, 1944) (70, 1944)\n"
     ]
    }
   ],
   "source": [
    "manage_data.process_features(train_data_dir, 5, train_features_dir)\n",
    "manage_data.process_features(test_data_dir, 5, test_features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train_filepaths', train_features.shape)\n",
    "# print('test_filepaths', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataset.take(1):  # Take one batch to print its shape\n",
    "#     images_batch, labels_batch = batch\n",
    "#     print(\"Images batch shape:\", images_batch.shape)\n",
    "#     print(\"Labels batch shape:\", labels_batch.shape)\n",
    "# for batch in test_dataset.take(1):  # Take one batch to print its shape\n",
    "#     images_batch, labels_batch = batch\n",
    "#     print(\"Images batch shape:\", images_batch.shape)\n",
    "#     print(\"Labels batch shape:\", labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sequence in manage_data.train_dataset:\n",
    "#     input_data, labels = sequence  # Unpack the tuple\n",
    "#     # for sequence2 in input_data:\n",
    "#     #     print('sequence=', sequence2.shape)\n",
    "#     print(\"Images batch shape:\", input_data.shape)\n",
    "#     print(\"Labels batch shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences = []\n",
    "# train_labels = []\n",
    "# val_sequences = []\n",
    "# val_labels = []\n",
    "\n",
    "# # train_dataset - (None, [8, 5, 224, 224, 3],[8])\n",
    "# # above one is a simplified version of a more complex tensor\n",
    "# for sequence, label in train_dataset:\n",
    "#     # sequence- (8, 5, 224, 224, 3)\n",
    "#     # label - (8)\n",
    "#     processed_sequence = ml.process_batch(sequence)\n",
    "#     train_sequences.append(processed_sequence)\n",
    "#     train_labels.append(label)\n",
    "\n",
    "# for sequence, label in test_dataset:\n",
    "#     processed_sequence = ml.process_batch(sequence)\n",
    "#     val_sequences.append(processed_sequence)\n",
    "#     train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_sequences=np.array(train_sequences)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print('train_sequences=', train_sequences.shape)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print('train_labels=', train_labels.shape)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m ml\u001b[38;5;241m.\u001b[39mtrain(\u001b[43mtrain_features\u001b[49m, train_labels, test_features, test_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "# train_sequences=np.array(train_sequences)\n",
    "# print('train_sequences=', train_sequences.shape)\n",
    "# print('train_labels=', train_labels.shape)\n",
    "\n",
    "\n",
    "ml.train(train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the model from the .h5 file\n",
    "# model = load_model('model_vgg.h5')\n",
    "# model.evaluate(manage_data2.test_dataset)\n",
    "\n",
    "# # Print the model summary to confirm it was loaded correctly\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
