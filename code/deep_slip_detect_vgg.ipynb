{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is demo code on to check how preprocess the images for vgg network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    manage_data = Manage_data()\n",
    "    image_path = os.path.join(manage_data.data_dir,'6/6.jpg')\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image to get a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Resize the image to the desired input size\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    # Convert image to a float32 tensor and preprocess it for VGG16\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_input(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224, 3])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Manage_data():\n",
    "    def __init__(self):\n",
    "        data_dir='/home/rag-tt/workspace/tactile_images/'\n",
    "        self.data_dir= pathlib.Path(data_dir)\n",
    "\n",
    "    def find_no_of_images(self, obj_id):\n",
    "        image_dir = os.path.join(self.data_dir, str(obj_id))\n",
    "        image_dir= pathlib.Path(image_dir)\n",
    "        no_of_images= len(list(image_dir.glob('*.jpg')))\n",
    "        return no_of_images\n",
    "    \n",
    "    \n",
    "\n",
    "    def parse_function_sequential(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [480, 640])  # Adjust size as needed\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image_resized)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "    \n",
    "    def parse_function_vgg(self, filenames, label):\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            image_string = tf.io.read_file(filename)\n",
    "            image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "            image_resized = tf.image.resize(image_decoded, [224, 224])  # Adjust size as needed\n",
    "            # Convert image to a float32 tensor and preprocess it for VGG16\n",
    "            image = tf.cast(image_resized, tf.float32)\n",
    "            image = preprocess_input(image)\n",
    "            # Ensure images are float32 and normalized between 0 and 1\n",
    "            images.append(image)\n",
    "        images = tf.stack(images)\n",
    "        return images, label\n",
    "    \n",
    "    def load_sequential_data(self, no_of_samples = 600):\n",
    "        file_paths = []\n",
    "        image_paths = []\n",
    "        sequential_image_paths = []\n",
    "        y = []\n",
    "        window_size = 5\n",
    "        for obj_id in range(no_of_samples):\n",
    "            no_of_images = self.find_no_of_images(obj_id)\n",
    "            if no_of_images < 40:\n",
    "                continue\n",
    "            \n",
    "            csv_path = os.path.join(self.data_dir, str(obj_id),'slip_log.csv')\n",
    "            label = np.genfromtxt(csv_path, delimiter=',', skip_header=1, usecols=1, dtype=None, encoding=None)\n",
    "            y.append(label[:-4])\n",
    "            \n",
    "            for img_id in range(no_of_images):\n",
    "                image_path = os.path.join(self.data_dir, str(obj_id), str(img_id)+ '.jpg')\n",
    "                image_paths.append(image_path)\n",
    "            for i in range(0, len(image_paths) - 4):  # Ensuring sequences of 5 images\n",
    "                row = image_paths[i:i+5]\n",
    "                sequential_image_paths.append(row)\n",
    "            image_paths = []\n",
    "\n",
    "        y = np.concatenate(y)\n",
    "        y = np.array(y)\n",
    "\n",
    "        file_paths = np.array(sequential_image_paths)\n",
    "\n",
    "        return file_paths, y\n",
    "    \n",
    "    def create_sequential_dataset(self,file_paths, labels):\n",
    "                # Create a TensorFlow dataset from the file paths and labels\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        \n",
    "        # # Map the function to each sequence of file paths and labels\n",
    "        # dataset = dataset.map(\n",
    "        #     lambda file_paths, label: self.parse_function_sequential(file_paths, label),\n",
    "        #     num_parallel_calls=tf.data.AUTOTUNE\n",
    "        # )\n",
    "        def wrapped_parse_function(filenames, label):\n",
    "            images, label = tf.py_function(func=self.parse_function_vgg, inp=[filenames, label], Tout=[tf.float32, tf.int64])\n",
    "            images.set_shape((5, 224, 224, 3))  # Explicitly set the shape\n",
    "            label.set_shape([])  # Explicitly set the shape for the label\n",
    "            return images, label\n",
    "        # # Map the parse_function to the dataset using tf.py_function\n",
    "        # dataset = dataset.map(lambda file_paths, label: tf.py_function(func=self.parse_function_sequential, inp=[file_paths, label], Tout=[tf.float32, tf.int64]),\n",
    "        #               num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        #     # Use tf.py_function to apply the parse_function\n",
    "        # def wrapped_parse_function(filenames, label):\n",
    "        #     return tf.py_function(func=self.parse_function_sequential, inp=[filenames, label], Tout=[tf.float32, tf.float32])\n",
    "\n",
    "        # dataset = dataset.map(wrapped_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        dataset = dataset.batch(32)  # Adjust batch size as needed\n",
    "        self.dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    def split_dataset(self, file_paths):\n",
    "        dataset_size = len(file_paths)\n",
    "        train_size = int(0.8 * dataset_size)\n",
    "        self.train_dataset = self.dataset.take(train_size)\n",
    "        self.val_dataset = self.dataset.skip(train_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_66 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138357544 (527.79 MB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import the VGG16 model with pre-trained weights\n",
    "# vgg_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "vgg_model = tf.keras.applications.VGG16(weights='imagenet')\n",
    "# Print the summary of the VGG16 model\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 224, 224, 3)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class create_network():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x =0\n",
    "    def cnn_lstm1(self):\n",
    "\n",
    "        # Define CNN model\n",
    "        cnn_model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(480, 640, 3)),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten()  # Flatten the spatial dimensions\n",
    "        ])\n",
    "\n",
    "        \n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64,input_shape=(5, 144768) ),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "        ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(cnn_model, input_shape=(5, 480, 640, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,144768))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        cp = ModelCheckpoint('model_vgg/',monitor='val_accuracy',save_best_only=True)\n",
    "        self.model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.00001),metrics=['accuracy'])\n",
    "        self.model.fit(train_dataset,validation_data=val_dataset, epochs=50, callbacks=[cp])\n",
    "\n",
    "    \n",
    "    def vgg_lstm(self):\n",
    "        # VGG16 model with pre-trained weights\n",
    "        #include top  false remove the final classification layer\n",
    "        vgg_model = tf.keras.applications.VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
    "        # Freeze the VGG16 layers if you don't want to train them\n",
    "        for layer in vgg_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Define CNN model\n",
    "        vgg_model_flatten = Sequential([\n",
    "            vgg_model,\n",
    "            Flatten(),  # Flatten the spatial dimensions\n",
    "            Dense(64, activation='relu')\n",
    "        ])\n",
    "\n",
    "        #25088 is the output of vff_model_flatten\n",
    "        # Define LSTM model\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(64, input_shape=(5, 64)),\n",
    "            Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "            Dense(8, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid'),\n",
    "                ])\n",
    "\n",
    "        # Combine CNN and LSTM models\n",
    "        self.model = Sequential([\n",
    "            TimeDistributed(vgg_model_flatten, input_shape=(5, 224, 224, 3)),  # Apply CNN to each frame in the sequence\n",
    "            (Reshape((5,64))),\n",
    "            lstm_model,\n",
    "        ])\n",
    "        vgg_model.summary()\n",
    "        vgg_model_flatten.summary()\n",
    "        self.model.summary()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_data = Manage_data()\n",
    "network = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66780, 5)\n",
      "Images batch shape: (32, 5, 224, 224, 3)\n",
      "Labels batch shape: (32,)\n",
      "filepaths size= (66780, 5)\n",
      "train set= <_TakeDataset element_spec=(TensorSpec(shape=(None, 5, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_67 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_30 (Flatten)        (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                1605696   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16320384 (62.26 MB)\n",
      "Trainable params: 1605696 (6.13 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_20 (TimeD  (None, 5, 64)             16320384  \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " reshape_20 (Reshape)        (None, 5, 64)             0         \n",
      "                                                                 \n",
      " sequential_71 (Sequential)  (None, 1)                 33553     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16353937 (62.39 MB)\n",
      "Trainable params: 1639249 (6.25 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.5584WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1316s 629ms/step - loss: 0.6980 - accuracy: 0.5584\n",
      "Epoch 2/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.5839WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1312s 629ms/step - loss: 0.6795 - accuracy: 0.5839\n",
      "Epoch 3/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.5938WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.6692 - accuracy: 0.5938\n",
      "Epoch 4/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.6038WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1311s 628ms/step - loss: 0.6571 - accuracy: 0.6038\n",
      "Epoch 5/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.6135WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1314s 629ms/step - loss: 0.6477 - accuracy: 0.6135\n",
      "Epoch 6/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.6232WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1308s 627ms/step - loss: 0.6319 - accuracy: 0.6232\n",
      "Epoch 7/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.6373WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.6191 - accuracy: 0.6373\n",
      "Epoch 8/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.6584WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1308s 626ms/step - loss: 0.6022 - accuracy: 0.6584\n",
      "Epoch 9/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.6655WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.5881 - accuracy: 0.6655\n",
      "Epoch 10/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.6888WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1308s 627ms/step - loss: 0.5678 - accuracy: 0.6888\n",
      "Epoch 11/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.7005WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1311s 628ms/step - loss: 0.5511 - accuracy: 0.7005\n",
      "Epoch 12/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.7191WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.5323 - accuracy: 0.7191\n",
      "Epoch 13/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.7320WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1307s 626ms/step - loss: 0.5157 - accuracy: 0.7320\n",
      "Epoch 14/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.5004 - accuracy: 0.7486WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.5004 - accuracy: 0.7486\n",
      "Epoch 15/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.7605WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.4833 - accuracy: 0.7605\n",
      "Epoch 16/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.7727WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1310s 627ms/step - loss: 0.4702 - accuracy: 0.7727\n",
      "Epoch 17/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4553 - accuracy: 0.7863WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1310s 628ms/step - loss: 0.4553 - accuracy: 0.7863\n",
      "Epoch 18/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.7956WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1308s 626ms/step - loss: 0.4412 - accuracy: 0.7956\n",
      "Epoch 19/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8036WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.4277 - accuracy: 0.8036\n",
      "Epoch 20/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8169WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1310s 627ms/step - loss: 0.4143 - accuracy: 0.8169\n",
      "Epoch 21/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.4048 - accuracy: 0.8201WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1307s 626ms/step - loss: 0.4048 - accuracy: 0.8201\n",
      "Epoch 22/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8281WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1302s 624ms/step - loss: 0.3910 - accuracy: 0.8281\n",
      "Epoch 23/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8356WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.3812 - accuracy: 0.8356\n",
      "Epoch 24/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3744 - accuracy: 0.8401WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1310s 628ms/step - loss: 0.3744 - accuracy: 0.8401\n",
      "Epoch 25/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8489WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1312s 629ms/step - loss: 0.3632 - accuracy: 0.8489\n",
      "Epoch 26/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8515WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.3609 - accuracy: 0.8515\n",
      "Epoch 27/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.8546WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1312s 629ms/step - loss: 0.3522 - accuracy: 0.8546\n",
      "Epoch 28/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.8614WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1312s 629ms/step - loss: 0.3460 - accuracy: 0.8614\n",
      "Epoch 29/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8577WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1316s 630ms/step - loss: 0.3495 - accuracy: 0.8577\n",
      "Epoch 30/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8681WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1313s 629ms/step - loss: 0.3312 - accuracy: 0.8681\n",
      "Epoch 31/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.8697WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1314s 630ms/step - loss: 0.3254 - accuracy: 0.8697\n",
      "Epoch 32/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.8761WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1313s 629ms/step - loss: 0.3194 - accuracy: 0.8761\n",
      "Epoch 33/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8696WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1313s 629ms/step - loss: 0.3353 - accuracy: 0.8696\n",
      "Epoch 34/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8789WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1311s 628ms/step - loss: 0.3184 - accuracy: 0.8789\n",
      "Epoch 35/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.8790WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1311s 628ms/step - loss: 0.3139 - accuracy: 0.8790\n",
      "Epoch 36/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.8857WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1312s 629ms/step - loss: 0.3044 - accuracy: 0.8857\n",
      "Epoch 37/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8881WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 2097s 1s/step - loss: 0.2976 - accuracy: 0.8881\n",
      "Epoch 38/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8834WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1569s 752ms/step - loss: 0.3031 - accuracy: 0.8834\n",
      "Epoch 39/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8746WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1885s 903ms/step - loss: 0.3128 - accuracy: 0.8746\n",
      "Epoch 40/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.8870WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1387s 665ms/step - loss: 0.2979 - accuracy: 0.8870\n",
      "Epoch 41/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.8938WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.2835 - accuracy: 0.8938\n",
      "Epoch 42/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8883WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1309s 627ms/step - loss: 0.2914 - accuracy: 0.8883\n",
      "Epoch 43/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.8842WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1508s 723ms/step - loss: 0.2998 - accuracy: 0.8842\n",
      "Epoch 44/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8893WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1460s 700ms/step - loss: 0.2856 - accuracy: 0.8893\n",
      "Epoch 45/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.8897WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 1348s 646ms/step - loss: 0.2847 - accuracy: 0.8897\n",
      "Epoch 46/50\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.8920WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "2087/2087 [==============================] - 2676s 1s/step - loss: 0.2776 - accuracy: 0.8920\n",
      "Epoch 47/50\n",
      " 958/2087 [============>.................] - ETA: 12:24 - loss: 0.2936 - accuracy: 0.8895"
     ]
    }
   ],
   "source": [
    "# creats a numpy array of images, then it lumps the images to together in batch of 5 for lstm\n",
    "#(None,5)\n",
    "filepaths, label = manage_data.load_sequential_data()\n",
    "\n",
    "\n",
    "# creates a tensor called dataset which contains the images. \n",
    "#The images are not loaded and stored in the seperate memory, it uses the existing images instead\n",
    "#This saves time and memory\n",
    "#(None,5,240,240,3)\n",
    "print(filepaths.shape)\n",
    "manage_data.create_sequential_dataset(filepaths,label)\n",
    "for batch in manage_data.dataset.take(1):  # Take one batch to print its shape\n",
    "    images_batch, labels_batch = batch\n",
    "    print(\"Images batch shape:\", images_batch.shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.shape)\n",
    "manage_data.split_dataset(filepaths)\n",
    "print('filepaths size=',filepaths.shape)\n",
    "#creates a combined network of cnn and lstm\n",
    "print('train set=',manage_data.train_dataset)\n",
    "network.vgg_lstm()\n",
    "network.train(manage_data.train_dataset, manage_data.val_dataset)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "network.model.save('model_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
